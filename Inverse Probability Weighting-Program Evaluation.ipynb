{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using IPW to evaluate an innovative secondary education program\n",
    "# Jiwon Son\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>WLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>c08_zmath</td>    <th>  R-squared:         </th> <td>   0.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th> <td>   0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   218.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Feb 2018</td> <th>  Prob (F-statistic):</th> <td>1.39e-66</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:57:59</td>     <th>  Log-Likelihood:    </th> <td> -965.57</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   713</td>      <th>  AIC:               </th> <td>   2013.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   672</td>      <th>  BIC:               </th> <td>   2200.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    40</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>cluster</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th> <td>   -0.2664</td> <td>    0.382</td> <td>   -0.697</td> <td> 0.488</td> <td>   -1.027</td> <td>    0.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat</th>      <td>    0.0416</td> <td>    0.116</td> <td>    0.358</td> <td> 0.721</td> <td>   -0.190</td> <td>    0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_2</th>     <td>    0.1473</td> <td>    0.456</td> <td>    0.323</td> <td> 0.747</td> <td>   -0.760</td> <td>    1.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_3</th>     <td>    1.2714</td> <td>    0.384</td> <td>    3.307</td> <td> 0.001</td> <td>    0.506</td> <td>    2.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_6</th>     <td>    0.3375</td> <td>    0.439</td> <td>    0.769</td> <td> 0.444</td> <td>   -0.536</td> <td>    1.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_7</th>     <td>    0.0137</td> <td>    0.384</td> <td>    0.036</td> <td> 0.972</td> <td>   -0.750</td> <td>    0.778</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_9</th>     <td>    0.4265</td> <td>    0.394</td> <td>    1.082</td> <td> 0.283</td> <td>   -0.358</td> <td>    1.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_10</th>    <td>    0.7381</td> <td>    0.410</td> <td>    1.801</td> <td> 0.076</td> <td>   -0.078</td> <td>    1.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_12</th>    <td>    0.6417</td> <td>    0.783</td> <td>    0.820</td> <td> 0.415</td> <td>   -0.917</td> <td>    2.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_13</th>    <td>    0.8942</td> <td>    0.381</td> <td>    2.345</td> <td> 0.022</td> <td>    0.135</td> <td>    1.653</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_15</th>    <td>    0.9155</td> <td>    0.432</td> <td>    2.120</td> <td> 0.037</td> <td>    0.056</td> <td>    1.775</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_16</th>    <td>   -0.3223</td> <td>    0.478</td> <td>   -0.674</td> <td> 0.502</td> <td>   -1.275</td> <td>    0.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_17</th>    <td>    0.1861</td> <td>    0.377</td> <td>    0.493</td> <td> 0.623</td> <td>   -0.565</td> <td>    0.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_18</th>    <td>   -1.0858</td> <td>    0.378</td> <td>   -2.872</td> <td> 0.005</td> <td>   -1.838</td> <td>   -0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_19</th>    <td>   -0.2624</td> <td>    0.377</td> <td>   -0.695</td> <td> 0.489</td> <td>   -1.013</td> <td>    0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_20</th>    <td>    0.5475</td> <td>    0.635</td> <td>    0.862</td> <td> 0.391</td> <td>   -0.717</td> <td>    1.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_21</th>    <td>   -0.2590</td> <td>    0.532</td> <td>   -0.487</td> <td> 0.627</td> <td>   -1.317</td> <td>    0.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_22</th>    <td>    0.5116</td> <td>    0.396</td> <td>    1.293</td> <td> 0.200</td> <td>   -0.276</td> <td>    1.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_23</th>    <td>   -0.3545</td> <td>    0.386</td> <td>   -0.920</td> <td> 0.361</td> <td>   -1.122</td> <td>    0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_24</th>    <td>    0.7272</td> <td>    0.382</td> <td>    1.904</td> <td> 0.061</td> <td>   -0.033</td> <td>    1.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_27</th>    <td>   -0.0439</td> <td>    0.624</td> <td>   -0.070</td> <td> 0.944</td> <td>   -1.286</td> <td>    1.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_30</th>    <td>    0.5760</td> <td>    0.398</td> <td>    1.446</td> <td> 0.152</td> <td>   -0.217</td> <td>    1.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_31</th>    <td>    0.2214</td> <td>    0.398</td> <td>    0.557</td> <td> 0.579</td> <td>   -0.570</td> <td>    1.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_33</th>    <td>    0.4236</td> <td>    0.376</td> <td>    1.126</td> <td> 0.264</td> <td>   -0.325</td> <td>    1.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_35</th>    <td>   -0.0452</td> <td>    0.376</td> <td>   -0.120</td> <td> 0.904</td> <td>   -0.793</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_36</th>    <td>   -0.1423</td> <td>    0.376</td> <td>   -0.378</td> <td> 0.706</td> <td>   -0.891</td> <td>    0.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_37</th>    <td>   -0.2757</td> <td>    0.573</td> <td>   -0.481</td> <td> 0.632</td> <td>   -1.417</td> <td>    0.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_38</th>    <td>    0.5423</td> <td>    0.686</td> <td>    0.791</td> <td> 0.432</td> <td>   -0.823</td> <td>    1.908</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_39</th>    <td>    1.2330</td> <td>    0.521</td> <td>    2.365</td> <td> 0.020</td> <td>    0.195</td> <td>    2.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_40</th>    <td>    0.4963</td> <td>    0.515</td> <td>    0.964</td> <td> 0.338</td> <td>   -0.528</td> <td>    1.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_42</th>    <td>   -0.0825</td> <td>    0.372</td> <td>   -0.221</td> <td> 0.825</td> <td>   -0.823</td> <td>    0.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_43</th>    <td>    0.6536</td> <td>    0.369</td> <td>    1.770</td> <td> 0.081</td> <td>   -0.082</td> <td>    1.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_45</th>    <td>    0.6193</td> <td>    0.925</td> <td>    0.670</td> <td> 0.505</td> <td>   -1.221</td> <td>    2.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_47</th>    <td>    0.9833</td> <td>    0.377</td> <td>    2.606</td> <td> 0.011</td> <td>    0.232</td> <td>    1.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_48</th>    <td>    0.4174</td> <td>    0.378</td> <td>    1.104</td> <td> 0.273</td> <td>   -0.335</td> <td>    1.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_50</th>    <td>    0.0831</td> <td>    0.378</td> <td>    0.220</td> <td> 0.826</td> <td>   -0.669</td> <td>    0.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_51</th>    <td>    0.3482</td> <td>    0.581</td> <td>    0.600</td> <td> 0.550</td> <td>   -0.807</td> <td>    1.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_52</th>    <td>    0.1183</td> <td>    0.491</td> <td>    0.241</td> <td> 0.810</td> <td>   -0.859</td> <td>    1.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_53</th>    <td>    0.3255</td> <td>    0.384</td> <td>    0.848</td> <td> 0.399</td> <td>   -0.439</td> <td>    1.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_55</th>    <td>    0.4557</td> <td>    0.375</td> <td>    1.216</td> <td> 0.228</td> <td>   -0.290</td> <td>    1.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_56</th>    <td>    0.3141</td> <td>    0.499</td> <td>    0.629</td> <td> 0.531</td> <td>   -0.680</td> <td>    1.308</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.631</td> <th>  Durbin-Watson:     </th> <td>   1.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.442</td> <th>  Jarque-Bera (JB):  </th> <td>   1.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.043</td> <th>  Prob(JB):          </th> <td>   0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.209</td> <th>  Cond. No.          </th> <td>    32.8</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            WLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              c08_zmath   R-squared:                       0.238\n",
       "Model:                            WLS   Adj. R-squared:                  0.193\n",
       "Method:                 Least Squares   F-statistic:                     218.0\n",
       "Date:                Fri, 16 Feb 2018   Prob (F-statistic):           1.39e-66\n",
       "Time:                        16:57:59   Log-Likelihood:                -965.57\n",
       "No. Observations:                 713   AIC:                             2013.\n",
       "Df Residuals:                     672   BIC:                             2200.\n",
       "Df Model:                          40                                         \n",
       "Covariance Type:              cluster                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "constant      -0.2664      0.382     -0.697      0.488      -1.027       0.494\n",
       "sat            0.0416      0.116      0.358      0.721      -0.190       0.273\n",
       "mp_2           0.1473      0.456      0.323      0.747      -0.760       1.054\n",
       "mp_3           1.2714      0.384      3.307      0.001       0.506       2.036\n",
       "mp_6           0.3375      0.439      0.769      0.444      -0.536       1.211\n",
       "mp_7           0.0137      0.384      0.036      0.972      -0.750       0.778\n",
       "mp_9           0.4265      0.394      1.082      0.283      -0.358       1.211\n",
       "mp_10          0.7381      0.410      1.801      0.076      -0.078       1.554\n",
       "mp_12          0.6417      0.783      0.820      0.415      -0.917       2.200\n",
       "mp_13          0.8942      0.381      2.345      0.022       0.135       1.653\n",
       "mp_15          0.9155      0.432      2.120      0.037       0.056       1.775\n",
       "mp_16         -0.3223      0.478     -0.674      0.502      -1.275       0.630\n",
       "mp_17          0.1861      0.377      0.493      0.623      -0.565       0.937\n",
       "mp_18         -1.0858      0.378     -2.872      0.005      -1.838      -0.333\n",
       "mp_19         -0.2624      0.377     -0.695      0.489      -1.013       0.489\n",
       "mp_20          0.5475      0.635      0.862      0.391      -0.717       1.812\n",
       "mp_21         -0.2590      0.532     -0.487      0.627      -1.317       0.799\n",
       "mp_22          0.5116      0.396      1.293      0.200      -0.276       1.299\n",
       "mp_23         -0.3545      0.386     -0.920      0.361      -1.122       0.413\n",
       "mp_24          0.7272      0.382      1.904      0.061      -0.033       1.487\n",
       "mp_27         -0.0439      0.624     -0.070      0.944      -1.286       1.198\n",
       "mp_30          0.5760      0.398      1.446      0.152      -0.217       1.369\n",
       "mp_31          0.2214      0.398      0.557      0.579      -0.570       1.013\n",
       "mp_33          0.4236      0.376      1.126      0.264      -0.325       1.173\n",
       "mp_35         -0.0452      0.376     -0.120      0.904      -0.793       0.702\n",
       "mp_36         -0.1423      0.376     -0.378      0.706      -0.891       0.607\n",
       "mp_37         -0.2757      0.573     -0.481      0.632      -1.417       0.865\n",
       "mp_38          0.5423      0.686      0.791      0.432      -0.823       1.908\n",
       "mp_39          1.2330      0.521      2.365      0.020       0.195       2.271\n",
       "mp_40          0.4963      0.515      0.964      0.338      -0.528       1.521\n",
       "mp_42         -0.0825      0.372     -0.221      0.825      -0.823       0.659\n",
       "mp_43          0.6536      0.369      1.770      0.081      -0.082       1.389\n",
       "mp_45          0.6193      0.925      0.670      0.505      -1.221       2.460\n",
       "mp_47          0.9833      0.377      2.606      0.011       0.232       1.734\n",
       "mp_48          0.4174      0.378      1.104      0.273      -0.335       1.170\n",
       "mp_50          0.0831      0.378      0.220      0.826      -0.669       0.835\n",
       "mp_51          0.3482      0.581      0.600      0.550      -0.807       1.504\n",
       "mp_52          0.1183      0.491      0.241      0.810      -0.859       1.096\n",
       "mp_53          0.3255      0.384      0.848      0.399      -0.439       1.090\n",
       "mp_55          0.4557      0.375      1.216      0.228      -0.290       1.202\n",
       "mp_56          0.3141      0.499      0.629      0.531      -0.680       1.308\n",
       "==============================================================================\n",
       "Omnibus:                        1.631   Durbin-Watson:                   1.876\n",
       "Prob(Omnibus):                  0.442   Jarque-Bera (JB):                1.521\n",
       "Skew:                           0.043   Prob(JB):                        0.467\n",
       "Kurtosis:                       3.209   Cond. No.                         32.8\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are robust tocluster correlation (cluster)\n",
       "\"\"\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load core data science libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read in tab delimited dataset into a pandas dataframe\n",
    "col_dtypes = {\"c10_zmath\" : float , \"wgt10_math\" : float , \"sat \" : int , \"constant \" : int , \"c08_zlang \" : float , \"c08_zmath\" : float , \"feeder_school\" : int, \"m_id_pairs\" : int}\n",
    "df = pd.read_csv(\"/Users/math_10.out.txt\", dtype = col_dtypes, na_values=\"\", engine=\"c\", sep = \"\\t\", encoding = \"utf−8\")\n",
    "# Construct a list of all matched SAT−CEB village pairs in the dataset\n",
    "included_pairs = sorted (df[\"m_id_pairs\"].unique ())\n",
    "# Form dummies for included matched SAT/CEB pairs\n",
    "pair_dums = pd.get_dummies(df[\"m_id_pairs\"].astype(\"category\"), prefix=\"mp\")\n",
    "# Concatenate matched pair dummies onto dataframe \n",
    "df = pd. concat ([ df , pair_dums ] , axis=1)\n",
    "# Construct outcome vector, design matrix, and test instrument inverse weights\n",
    "Y= df[\"c08_zmath\"] #Outcome\n",
    "test_wgt = 1./df[\"wgt10_math\"] # Test instrument weights \n",
    "X= df[[\"constant\",\"sat\"]] # Design matrix\n",
    "\n",
    "X= pd.concat([X, df.loc[:,\"mp_\" + str(included_pairs[0]) : \"mp_\"+ str(included_pairs[-2])]], axis=1)\n",
    "#NOTE: omit last matched pair to avoid \"dummy variable trap\"\n",
    "# Compute weighted least squares fit\n",
    "# NOTE: cluster−robust standard errors\n",
    "wls = sm.WLS(Y,X,weights=test_wgt).fit(cov_type=\"cluster\",cov_kwds={\"groups\": df[\"feeder_school\"]},use_t=True)\n",
    "wls.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Interpret the coefficient on sat in light of the research design described by McEwan et al. (2015). What is accomplished by weighting by the inverse of wgt10_math weights?\n",
    "\n",
    "Answer: The coefficient on sat is 0.0416 with standard error 0.116. This means that students who live in\n",
    "  SAT village in 2008 are on average likely to receive a math grade 0.04 standard deviations higher than the baseline,   compared to those who live in CEB village.\n",
    "  Through weighting by the inverse of the wgt10_math weights, we can upweight the subpopulation\n",
    "  who took in-home test (either math or language test) in comparison to the subpopulation who took the original test     at school (both math and language tests) and compare these two subpoplations on the same scale. We can do this         because the students who took in-home test and the group of students who took the test in school are equally           important for analysis and we want to make sure that the regression result is not biased towards the group of         students who took the original test in school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>WLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>c10_zmath</td>    <th>  R-squared:         </th> <td>   0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th> <td>   0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   48.02</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Feb 2018</td> <th>  Prob (F-statistic):</th> <td>2.02e-41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:57:59</td>     <th>  Log-Likelihood:    </th> <td> -1067.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   713</td>      <th>  AIC:               </th> <td>   2217.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   672</td>      <th>  BIC:               </th> <td>   2404.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    40</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>cluster</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th> <td>   -0.1922</td> <td>    0.206</td> <td>   -0.934</td> <td> 0.353</td> <td>   -0.602</td> <td>    0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat</th>      <td>    0.1547</td> <td>    0.099</td> <td>    1.560</td> <td> 0.123</td> <td>   -0.043</td> <td>    0.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_2</th>     <td>    0.0426</td> <td>    0.224</td> <td>    0.191</td> <td> 0.849</td> <td>   -0.402</td> <td>    0.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_3</th>     <td>    0.9798</td> <td>    0.226</td> <td>    4.334</td> <td> 0.000</td> <td>    0.530</td> <td>    1.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_6</th>     <td>    0.8838</td> <td>    0.211</td> <td>    4.197</td> <td> 0.000</td> <td>    0.465</td> <td>    1.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_7</th>     <td>    0.4992</td> <td>    0.314</td> <td>    1.589</td> <td> 0.116</td> <td>   -0.126</td> <td>    1.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_9</th>     <td>    1.0689</td> <td>    0.243</td> <td>    4.398</td> <td> 0.000</td> <td>    0.585</td> <td>    1.553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_10</th>    <td>    0.2537</td> <td>    0.334</td> <td>    0.759</td> <td> 0.450</td> <td>   -0.411</td> <td>    0.919</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_12</th>    <td>    0.5964</td> <td>    0.297</td> <td>    2.009</td> <td> 0.048</td> <td>    0.006</td> <td>    1.187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_13</th>    <td>    0.9082</td> <td>    0.195</td> <td>    4.661</td> <td> 0.000</td> <td>    0.520</td> <td>    1.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_15</th>    <td>    0.7975</td> <td>    0.219</td> <td>    3.637</td> <td> 0.000</td> <td>    0.361</td> <td>    1.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_16</th>    <td>    0.3924</td> <td>    0.205</td> <td>    1.915</td> <td> 0.059</td> <td>   -0.015</td> <td>    0.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_17</th>    <td>    0.4594</td> <td>    0.203</td> <td>    2.267</td> <td> 0.026</td> <td>    0.056</td> <td>    0.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_18</th>    <td>   -0.6551</td> <td>    0.239</td> <td>   -2.743</td> <td> 0.008</td> <td>   -1.130</td> <td>   -0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_19</th>    <td>    0.6984</td> <td>    0.278</td> <td>    2.512</td> <td> 0.014</td> <td>    0.145</td> <td>    1.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_20</th>    <td>    0.9563</td> <td>    0.287</td> <td>    3.333</td> <td> 0.001</td> <td>    0.385</td> <td>    1.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_21</th>    <td>    0.1826</td> <td>    0.672</td> <td>    0.272</td> <td> 0.787</td> <td>   -1.155</td> <td>    1.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_22</th>    <td>    0.0197</td> <td>    0.246</td> <td>    0.080</td> <td> 0.936</td> <td>   -0.469</td> <td>    0.509</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_23</th>    <td>    0.2060</td> <td>    0.317</td> <td>    0.650</td> <td> 0.517</td> <td>   -0.425</td> <td>    0.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_24</th>    <td>    0.6934</td> <td>    0.207</td> <td>    3.344</td> <td> 0.001</td> <td>    0.281</td> <td>    1.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_27</th>    <td>    0.3406</td> <td>    0.238</td> <td>    1.428</td> <td> 0.157</td> <td>   -0.134</td> <td>    0.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_30</th>    <td>    0.7343</td> <td>    0.646</td> <td>    1.136</td> <td> 0.259</td> <td>   -0.552</td> <td>    2.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_31</th>    <td>    1.1224</td> <td>    0.250</td> <td>    4.481</td> <td> 0.000</td> <td>    0.624</td> <td>    1.621</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_33</th>    <td>    0.8081</td> <td>    0.346</td> <td>    2.335</td> <td> 0.022</td> <td>    0.119</td> <td>    1.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_35</th>    <td>   -0.1022</td> <td>    0.231</td> <td>   -0.442</td> <td> 0.660</td> <td>   -0.563</td> <td>    0.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_36</th>    <td>    0.4726</td> <td>    0.199</td> <td>    2.373</td> <td> 0.020</td> <td>    0.076</td> <td>    0.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_37</th>    <td>    0.1260</td> <td>    0.438</td> <td>    0.288</td> <td> 0.774</td> <td>   -0.746</td> <td>    0.998</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_38</th>    <td>    1.0243</td> <td>    0.446</td> <td>    2.298</td> <td> 0.024</td> <td>    0.137</td> <td>    1.911</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_39</th>    <td>    0.7930</td> <td>    0.202</td> <td>    3.928</td> <td> 0.000</td> <td>    0.391</td> <td>    1.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_40</th>    <td>    0.7258</td> <td>    0.270</td> <td>    2.691</td> <td> 0.009</td> <td>    0.189</td> <td>    1.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_42</th>    <td>   -0.2063</td> <td>    0.198</td> <td>   -1.042</td> <td> 0.300</td> <td>   -0.600</td> <td>    0.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_43</th>    <td>    0.8671</td> <td>    0.228</td> <td>    3.804</td> <td> 0.000</td> <td>    0.413</td> <td>    1.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_45</th>    <td>    0.9330</td> <td>    0.873</td> <td>    1.069</td> <td> 0.288</td> <td>   -0.804</td> <td>    2.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_47</th>    <td>    0.3070</td> <td>    0.422</td> <td>    0.727</td> <td> 0.469</td> <td>   -0.534</td> <td>    1.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_48</th>    <td>   -0.0097</td> <td>    0.335</td> <td>   -0.029</td> <td> 0.977</td> <td>   -0.677</td> <td>    0.658</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_50</th>    <td>    0.6283</td> <td>    0.331</td> <td>    1.896</td> <td> 0.062</td> <td>   -0.031</td> <td>    1.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_51</th>    <td>    0.5546</td> <td>    0.204</td> <td>    2.721</td> <td> 0.008</td> <td>    0.149</td> <td>    0.960</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_52</th>    <td>    0.5983</td> <td>    0.586</td> <td>    1.022</td> <td> 0.310</td> <td>   -0.567</td> <td>    1.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_53</th>    <td>    0.5117</td> <td>    0.347</td> <td>    1.475</td> <td> 0.144</td> <td>   -0.179</td> <td>    1.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_55</th>    <td>    0.8047</td> <td>    0.225</td> <td>    3.583</td> <td> 0.001</td> <td>    0.358</td> <td>    1.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_56</th>    <td>    0.5474</td> <td>    0.596</td> <td>    0.919</td> <td> 0.361</td> <td>   -0.639</td> <td>    1.733</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.516</td> <th>  Durbin-Watson:     </th> <td>   1.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.172</td> <th>  Jarque-Bera (JB):  </th> <td>   3.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.164</td> <th>  Prob(JB):          </th> <td>   0.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.075</td> <th>  Cond. No.          </th> <td>    32.8</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            WLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              c10_zmath   R-squared:                       0.146\n",
       "Model:                            WLS   Adj. R-squared:                  0.095\n",
       "Method:                 Least Squares   F-statistic:                     48.02\n",
       "Date:                Fri, 16 Feb 2018   Prob (F-statistic):           2.02e-41\n",
       "Time:                        16:57:59   Log-Likelihood:                -1067.3\n",
       "No. Observations:                 713   AIC:                             2217.\n",
       "Df Residuals:                     672   BIC:                             2404.\n",
       "Df Model:                          40                                         \n",
       "Covariance Type:              cluster                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "constant      -0.1922      0.206     -0.934      0.353      -0.602       0.218\n",
       "sat            0.1547      0.099      1.560      0.123      -0.043       0.352\n",
       "mp_2           0.0426      0.224      0.191      0.849      -0.402       0.488\n",
       "mp_3           0.9798      0.226      4.334      0.000       0.530       1.430\n",
       "mp_6           0.8838      0.211      4.197      0.000       0.465       1.303\n",
       "mp_7           0.4992      0.314      1.589      0.116      -0.126       1.125\n",
       "mp_9           1.0689      0.243      4.398      0.000       0.585       1.553\n",
       "mp_10          0.2537      0.334      0.759      0.450      -0.411       0.919\n",
       "mp_12          0.5964      0.297      2.009      0.048       0.006       1.187\n",
       "mp_13          0.9082      0.195      4.661      0.000       0.520       1.296\n",
       "mp_15          0.7975      0.219      3.637      0.000       0.361       1.234\n",
       "mp_16          0.3924      0.205      1.915      0.059      -0.015       0.800\n",
       "mp_17          0.4594      0.203      2.267      0.026       0.056       0.863\n",
       "mp_18         -0.6551      0.239     -2.743      0.008      -1.130      -0.180\n",
       "mp_19          0.6984      0.278      2.512      0.014       0.145       1.252\n",
       "mp_20          0.9563      0.287      3.333      0.001       0.385       1.527\n",
       "mp_21          0.1826      0.672      0.272      0.787      -1.155       1.520\n",
       "mp_22          0.0197      0.246      0.080      0.936      -0.469       0.509\n",
       "mp_23          0.2060      0.317      0.650      0.517      -0.425       0.837\n",
       "mp_24          0.6934      0.207      3.344      0.001       0.281       1.106\n",
       "mp_27          0.3406      0.238      1.428      0.157      -0.134       0.815\n",
       "mp_30          0.7343      0.646      1.136      0.259      -0.552       2.020\n",
       "mp_31          1.1224      0.250      4.481      0.000       0.624       1.621\n",
       "mp_33          0.8081      0.346      2.335      0.022       0.119       1.497\n",
       "mp_35         -0.1022      0.231     -0.442      0.660      -0.563       0.358\n",
       "mp_36          0.4726      0.199      2.373      0.020       0.076       0.869\n",
       "mp_37          0.1260      0.438      0.288      0.774      -0.746       0.998\n",
       "mp_38          1.0243      0.446      2.298      0.024       0.137       1.911\n",
       "mp_39          0.7930      0.202      3.928      0.000       0.391       1.195\n",
       "mp_40          0.7258      0.270      2.691      0.009       0.189       1.263\n",
       "mp_42         -0.2063      0.198     -1.042      0.300      -0.600       0.188\n",
       "mp_43          0.8671      0.228      3.804      0.000       0.413       1.321\n",
       "mp_45          0.9330      0.873      1.069      0.288      -0.804       2.670\n",
       "mp_47          0.3070      0.422      0.727      0.469      -0.534       1.148\n",
       "mp_48         -0.0097      0.335     -0.029      0.977      -0.677       0.658\n",
       "mp_50          0.6283      0.331      1.896      0.062      -0.031       1.288\n",
       "mp_51          0.5546      0.204      2.721      0.008       0.149       0.960\n",
       "mp_52          0.5983      0.586      1.022      0.310      -0.567       1.764\n",
       "mp_53          0.5117      0.347      1.475      0.144      -0.179       1.202\n",
       "mp_55          0.8047      0.225      3.583      0.001       0.358       1.252\n",
       "mp_56          0.5474      0.596      0.919      0.361      -0.639       1.733\n",
       "==============================================================================\n",
       "Omnibus:                        3.516   Durbin-Watson:                   1.863\n",
       "Prob(Omnibus):                  0.172   Jarque-Bera (JB):                3.373\n",
       "Skew:                          -0.164   Prob(JB):                        0.185\n",
       "Kurtosis:                       3.075   Cond. No.                         32.8\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are robust tocluster correlation (cluster)\n",
       "\"\"\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3\n",
    "# Construct outcome vector\n",
    "Y1= df[\"c10_zmath\"] #Outcome\n",
    "# Compute weighted least squares fit, use design matrix and test instrument inverse weights from #2\n",
    "# NOTE: cluster−robust standard errors\n",
    "wls2 = sm.WLS(Y1,X,weights=test_wgt).fit(cov_type=\"cluster\",cov_kwds={\"groups\": df[\"feeder_school\"]},use_t=True)\n",
    "wls2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Interpret the coefficient on sat in light of the resaerch design described by McEwan et al. (2015)\n",
    "\n",
    "Answer: The coefficient on sat is 0.1547. This means that students who live in SAT village in 2010 are on average likely to receive a math test grade 0.15 standard deviations higher than the baseline, compared to those who live in CEB village."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>WLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>c10_zmath</td>    <th>  R-squared:         </th> <td>   0.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th> <td>   0.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   69.49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 16 Feb 2018</td> <th>  Prob (F-statistic):</th> <td>8.57e-48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:57:59</td>     <th>  Log-Likelihood:    </th> <td> -902.26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   713</td>      <th>  AIC:               </th> <td>   1891.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   670</td>      <th>  BIC:               </th> <td>   2087.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    42</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>cluster</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th>  <td>    0.0319</td> <td>    0.055</td> <td>    0.579</td> <td> 0.564</td> <td>   -0.078</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat</th>       <td>    0.1342</td> <td>    0.082</td> <td>    1.636</td> <td> 0.106</td> <td>   -0.029</td> <td>    0.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>c08_zmath</th> <td>    0.5124</td> <td>    0.056</td> <td>    9.173</td> <td> 0.000</td> <td>    0.401</td> <td>    0.624</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>c08_zlang</th> <td>    0.2577</td> <td>    0.049</td> <td>    5.264</td> <td> 0.000</td> <td>    0.160</td> <td>    0.355</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_2</th>      <td>   -0.1218</td> <td>    0.246</td> <td>   -0.495</td> <td> 0.622</td> <td>   -0.612</td> <td>    0.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_3</th>      <td>   -0.0918</td> <td>    0.087</td> <td>   -1.059</td> <td> 0.293</td> <td>   -0.264</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_6</th>      <td>    0.6063</td> <td>    0.276</td> <td>    2.198</td> <td> 0.031</td> <td>    0.057</td> <td>    1.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_7</th>      <td>    0.4544</td> <td>    0.165</td> <td>    2.758</td> <td> 0.007</td> <td>    0.126</td> <td>    0.782</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_9</th>      <td>    0.7510</td> <td>    0.048</td> <td>   15.508</td> <td> 0.000</td> <td>    0.655</td> <td>    0.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_10</th>     <td>   -0.3037</td> <td>    0.112</td> <td>   -2.703</td> <td> 0.008</td> <td>   -0.527</td> <td>   -0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_12</th>     <td>    0.2819</td> <td>    0.239</td> <td>    1.180</td> <td> 0.242</td> <td>   -0.194</td> <td>    0.757</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_13</th>     <td>    0.2218</td> <td>    0.097</td> <td>    2.296</td> <td> 0.024</td> <td>    0.030</td> <td>    0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_15</th>     <td>    0.2058</td> <td>    0.049</td> <td>    4.194</td> <td> 0.000</td> <td>    0.108</td> <td>    0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_16</th>     <td>    0.5164</td> <td>    0.263</td> <td>    1.965</td> <td> 0.053</td> <td>   -0.007</td> <td>    1.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_17</th>     <td>    0.3471</td> <td>    0.071</td> <td>    4.858</td> <td> 0.000</td> <td>    0.205</td> <td>    0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_18</th>     <td>    0.1698</td> <td>    0.133</td> <td>    1.272</td> <td> 0.207</td> <td>   -0.096</td> <td>    0.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_19</th>     <td>    0.7460</td> <td>    0.209</td> <td>    3.566</td> <td> 0.001</td> <td>    0.330</td> <td>    1.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_20</th>     <td>    0.4745</td> <td>    0.167</td> <td>    2.849</td> <td> 0.006</td> <td>    0.143</td> <td>    0.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_21</th>     <td>    0.2258</td> <td>    0.404</td> <td>    0.560</td> <td> 0.577</td> <td>   -0.577</td> <td>    1.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_22</th>     <td>   -0.4286</td> <td>    0.162</td> <td>   -2.645</td> <td> 0.010</td> <td>   -0.751</td> <td>   -0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_23</th>     <td>    0.3738</td> <td>    0.161</td> <td>    2.321</td> <td> 0.023</td> <td>    0.053</td> <td>    0.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_24</th>     <td>    0.2733</td> <td>    0.140</td> <td>    1.953</td> <td> 0.054</td> <td>   -0.005</td> <td>    0.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_27</th>     <td>    0.4040</td> <td>    0.154</td> <td>    2.629</td> <td> 0.010</td> <td>    0.098</td> <td>    0.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_30</th>     <td>    0.2758</td> <td>    0.473</td> <td>    0.583</td> <td> 0.562</td> <td>   -0.666</td> <td>    1.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_31</th>     <td>    0.9408</td> <td>    0.102</td> <td>    9.259</td> <td> 0.000</td> <td>    0.739</td> <td>    1.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_33</th>     <td>    0.4265</td> <td>    0.252</td> <td>    1.693</td> <td> 0.094</td> <td>   -0.075</td> <td>    0.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_35</th>     <td>    0.0006</td> <td>    0.083</td> <td>    0.007</td> <td> 0.995</td> <td>   -0.165</td> <td>    0.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_36</th>     <td>    0.5594</td> <td>    0.040</td> <td>   13.922</td> <td> 0.000</td> <td>    0.479</td> <td>    0.639</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_37</th>     <td>    0.1787</td> <td>    0.121</td> <td>    1.480</td> <td> 0.143</td> <td>   -0.062</td> <td>    0.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_38</th>     <td>    0.5310</td> <td>    0.051</td> <td>   10.413</td> <td> 0.000</td> <td>    0.429</td> <td>    0.632</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_39</th>     <td>   -0.1882</td> <td>    0.183</td> <td>   -1.028</td> <td> 0.307</td> <td>   -0.553</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_40</th>     <td>    0.3051</td> <td>    0.062</td> <td>    4.940</td> <td> 0.000</td> <td>    0.182</td> <td>    0.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_42</th>     <td>   -0.1855</td> <td>    0.100</td> <td>   -1.853</td> <td> 0.068</td> <td>   -0.385</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_43</th>     <td>    0.4251</td> <td>    0.141</td> <td>    3.005</td> <td> 0.004</td> <td>    0.144</td> <td>    0.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_45</th>     <td>    0.4729</td> <td>    0.251</td> <td>    1.884</td> <td> 0.063</td> <td>   -0.027</td> <td>    0.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_47</th>     <td>   -0.3124</td> <td>    0.253</td> <td>   -1.233</td> <td> 0.221</td> <td>   -0.817</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_48</th>     <td>   -0.3160</td> <td>    0.257</td> <td>   -1.230</td> <td> 0.223</td> <td>   -0.828</td> <td>    0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_50</th>     <td>    0.5660</td> <td>    0.235</td> <td>    2.408</td> <td> 0.018</td> <td>    0.098</td> <td>    1.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_51</th>     <td>    0.3310</td> <td>    0.402</td> <td>    0.823</td> <td> 0.413</td> <td>   -0.470</td> <td>    1.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_52</th>     <td>    0.4189</td> <td>    0.304</td> <td>    1.376</td> <td> 0.173</td> <td>   -0.187</td> <td>    1.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_53</th>     <td>    0.3497</td> <td>    0.206</td> <td>    1.701</td> <td> 0.093</td> <td>   -0.060</td> <td>    0.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_55</th>     <td>    0.6104</td> <td>    0.086</td> <td>    7.092</td> <td> 0.000</td> <td>    0.439</td> <td>    0.782</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mp_56</th>     <td>    0.2539</td> <td>    0.379</td> <td>    0.670</td> <td> 0.505</td> <td>   -0.501</td> <td>    1.008</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.179</td> <th>  Durbin-Watson:     </th> <td>   1.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  23.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.115</td> <th>  Prob(JB):          </th> <td>7.58e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.861</td> <th>  Cond. No.          </th> <td>    39.0</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            WLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              c10_zmath   R-squared:                       0.462\n",
       "Model:                            WLS   Adj. R-squared:                  0.429\n",
       "Method:                 Least Squares   F-statistic:                     69.49\n",
       "Date:                Fri, 16 Feb 2018   Prob (F-statistic):           8.57e-48\n",
       "Time:                        16:57:59   Log-Likelihood:                -902.26\n",
       "No. Observations:                 713   AIC:                             1891.\n",
       "Df Residuals:                     670   BIC:                             2087.\n",
       "Df Model:                          42                                         \n",
       "Covariance Type:              cluster                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "constant       0.0319      0.055      0.579      0.564      -0.078       0.142\n",
       "sat            0.1342      0.082      1.636      0.106      -0.029       0.297\n",
       "c08_zmath      0.5124      0.056      9.173      0.000       0.401       0.624\n",
       "c08_zlang      0.2577      0.049      5.264      0.000       0.160       0.355\n",
       "mp_2          -0.1218      0.246     -0.495      0.622      -0.612       0.368\n",
       "mp_3          -0.0918      0.087     -1.059      0.293      -0.264       0.081\n",
       "mp_6           0.6063      0.276      2.198      0.031       0.057       1.155\n",
       "mp_7           0.4544      0.165      2.758      0.007       0.126       0.782\n",
       "mp_9           0.7510      0.048     15.508      0.000       0.655       0.847\n",
       "mp_10         -0.3037      0.112     -2.703      0.008      -0.527      -0.080\n",
       "mp_12          0.2819      0.239      1.180      0.242      -0.194       0.757\n",
       "mp_13          0.2218      0.097      2.296      0.024       0.030       0.414\n",
       "mp_15          0.2058      0.049      4.194      0.000       0.108       0.303\n",
       "mp_16          0.5164      0.263      1.965      0.053      -0.007       1.039\n",
       "mp_17          0.3471      0.071      4.858      0.000       0.205       0.489\n",
       "mp_18          0.1698      0.133      1.272      0.207      -0.096       0.435\n",
       "mp_19          0.7460      0.209      3.566      0.001       0.330       1.162\n",
       "mp_20          0.4745      0.167      2.849      0.006       0.143       0.806\n",
       "mp_21          0.2258      0.404      0.560      0.577      -0.577       1.029\n",
       "mp_22         -0.4286      0.162     -2.645      0.010      -0.751      -0.106\n",
       "mp_23          0.3738      0.161      2.321      0.023       0.053       0.694\n",
       "mp_24          0.2733      0.140      1.953      0.054      -0.005       0.552\n",
       "mp_27          0.4040      0.154      2.629      0.010       0.098       0.710\n",
       "mp_30          0.2758      0.473      0.583      0.562      -0.666       1.217\n",
       "mp_31          0.9408      0.102      9.259      0.000       0.739       1.143\n",
       "mp_33          0.4265      0.252      1.693      0.094      -0.075       0.928\n",
       "mp_35          0.0006      0.083      0.007      0.995      -0.165       0.166\n",
       "mp_36          0.5594      0.040     13.922      0.000       0.479       0.639\n",
       "mp_37          0.1787      0.121      1.480      0.143      -0.062       0.419\n",
       "mp_38          0.5310      0.051     10.413      0.000       0.429       0.632\n",
       "mp_39         -0.1882      0.183     -1.028      0.307      -0.553       0.176\n",
       "mp_40          0.3051      0.062      4.940      0.000       0.182       0.428\n",
       "mp_42         -0.1855      0.100     -1.853      0.068      -0.385       0.014\n",
       "mp_43          0.4251      0.141      3.005      0.004       0.144       0.707\n",
       "mp_45          0.4729      0.251      1.884      0.063      -0.027       0.972\n",
       "mp_47         -0.3124      0.253     -1.233      0.221      -0.817       0.192\n",
       "mp_48         -0.3160      0.257     -1.230      0.223      -0.828       0.196\n",
       "mp_50          0.5660      0.235      2.408      0.018       0.098       1.034\n",
       "mp_51          0.3310      0.402      0.823      0.413      -0.470       1.132\n",
       "mp_52          0.4189      0.304      1.376      0.173      -0.187       1.025\n",
       "mp_53          0.3497      0.206      1.701      0.093      -0.060       0.759\n",
       "mp_55          0.6104      0.086      7.092      0.000       0.439       0.782\n",
       "mp_56          0.2539      0.379      0.670      0.505      -0.501       1.008\n",
       "==============================================================================\n",
       "Omnibus:                       14.179   Durbin-Watson:                   1.868\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               23.581\n",
       "Skew:                          -0.115   Prob(JB):                     7.58e-06\n",
       "Kurtosis:                       3.861   Cond. No.                         39.0\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are robust tocluster correlation (cluster)\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4\n",
    "# Add \"c08_zmath\" and \"c08_zlang\" as control variables in the WLS fit and compute the new WLS fit\n",
    "X1= df[[\"constant\",\"sat\",\"c08_zmath\",\"c08_zlang\"]]\n",
    "X1= pd.concat([X1, df.loc[:,\"mp_\" + str(included_pairs[0]) : \"mp_\"+ str(included_pairs[-2])]], axis=1)\n",
    "wls3 = sm.WLS(Y1,X1,weights=test_wgt).fit(cov_type=\"cluster\",cov_kwds={\"groups\": df[\"feeder_school\"]},use_t=True)\n",
    "wls3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Interpret the coefficient on sat.\n",
    "\n",
    "Ans: Controlling for c08_zmath and c08_zlang, we can expect to consider those students who\n",
    "     are inherently good separately from the effect of SAT on the student score, and separate that out\n",
    "     to better measure the effect of SAT on the score.\n",
    "     If the student lives in sat village in 2010, given that their math and langauge test scores in 2008 are                controlled, then we can expect the average treatment effect of SAT to be 0.1342, as shown in the regression            table.\n",
    "     To state this differently, students who live in sat village in 2010 are on average likely to \n",
    "     receive a math test grade 0.1342 standard deviations higher than the baseline, compared to those who live in CEB      village."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.627957\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c20255198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wMr83Nzrcr44gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c20244630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wPq6n2XCpcKOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c20304780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0BbhdXFsu5tAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c20201da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#5\n",
    "#Compute the logistic regression fit of sat onto a constant, the matched SAT-CEB pair dummies, and the two baseline test scores\n",
    "Y2= df[\"sat\"]\n",
    "X3= df[[\"constant\", \"c08_zmath\", \"c08_zlang\"]]\n",
    "X3= pd.concat([X3, df.loc[:,\"mp_\" + str(included_pairs[0]) : \"mp_\"+ str(included_pairs[-2])]], axis=1)\n",
    "lr=sm.Logit(Y2,X3).fit()\n",
    "lr.summary()\n",
    "\n",
    "#Compute the fitted propensity score values\n",
    "fitted=lr.fittedvalues.values\n",
    "Exp= np.exp(fitted)\n",
    "\n",
    "Propensity_Score= Exp/(1+Exp)\n",
    "\n",
    "#Plot the propensity scores for treated and controlled groups as a graphical evidence for overlap condition satisfied\n",
    "bins=np.linspace(0,1,20)\n",
    "\n",
    "TableSatProp = pd.DataFrame ({'sat':Y2,'e(x)':Propensity_Score})\n",
    "\n",
    "PropenT = TableSatProp[TableSatProp['sat']==1]\n",
    "['e(x)']\n",
    "PropenC = TableSatProp[TableSatProp['sat']==0]\n",
    "['e(x)']\n",
    "\n",
    "plt.hist(PropenT['e(x)'], bins)\n",
    "plt.show()\n",
    "plt.hist(PropenC['e(x)'], bins)\n",
    "plt.show()\n",
    "plt.hist(PropenT['e(x)'], bins)\n",
    "plt.hist(PropenC['e(x)'], bins)\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(PropenT['e(x)'],bins) \n",
    "sns.distplot(PropenC['e(x)'],bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Compute the propensity score values. Is the overlap condition satisfied? Why?\n",
    "\n",
    "Ans: The overlap condition is satisfied because the propensity scores for both treatment and control groups range      between 0 and 1. Based on the assumption that we need some treatment and control units for each value of the propensity score, we want to see if the propensity scores for the two groups overlap. If we plot the propensity score for each group, the two groups show a considerable amount of overlap. Therefore, we can compare the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#6\n",
    "#Compute the IPW weights for average treatment effect estimation\n",
    "V = (Y2/Propensity_Score)+((1-Y2)/(1-Propensity_Score))\n",
    "X4= df[[\"constant\", \"sat\"]]\n",
    "\n",
    "#Compute the weighted least squares fit of math test score in 2010 onto a constant and sat \n",
    "#using the IPW weights multiplied by test instrument weights\n",
    "#NOTE: cluster−robust standard errors\n",
    "wls4 = sm.WLS(Y1,X4,weights=test_wgt*V).fit(cov_type=\"cluster\", cov_kwds={\"groups\": df[\"feeder_school\"]},use_t=True)\n",
    "wls4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What is accomplished by using these weights. Interpret the coefficient on sat.\n",
    "\n",
    "Ans:  Weighting by the IPW weights*test_wgt, we can expect to upweight the underrepresneted group (those with low           propensity score), and thus adjust the average treatment effect of the independent variable by leveling the           probability that subgroups in SAT and CEB pairs are represented in the experiment.\n",
    "      This is because some subgroups of students who have certain covariate characteristics are more likely to be           treated than other subgroups in each pair.\n",
    "      The sat coefficient is 0.1602 with cluster-robust standard error 0.143. This means that\n",
    "      students who live in SAT village in 2010 are on average likely to receive a math test grade 0.16 standard             deviations higher than the baseline, compared to those who live in CEB village."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. What asdditoinal data would you have collected to answer the questions studied by McEwan et al. (2015) if provided the opportunity?\n",
    "\n",
    "Ans: If given the opportunity, I would have collected percentage of homes with internet connection in addition to percentage of homes with computer already collected. Also, I would have collected grandparents' level of education.   p value is too big to give a statistically significant result in almost every regression done so far, so we might want to include more control variables adjust for the possible omitted variable bias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
