{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Econoimcs 142 \n",
    "\n",
    "#Estimating Productivity Using OLS Regression/Panel Regression/Proxy Variable Regression\n",
    "#Jiwon Son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load core data science libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#1.1 Preparing the dataset\n",
    "# Read in tab-delimited dataset into a pandas dataframe\n",
    "col_dtypes = {\"gvkey\" : int, \"year\" : int, \"Y\" : float, \"K\" : float, \"L\" : float, \"M\" : float , \"VA\" : float, \"i\" : float, \"naics_4digit\" : int}\n",
    "\n",
    "df = pd.read_csv(\"/Users/davidson/.spyder-py3/mf_firms.out.txt\", dtype = col_dtypes, na_values=\"\", engine=\"c\", sep = \"\\t\", encoding = \"utf−8\")\n",
    "df.drop(['gvkey.1','year.1'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Obtain distinct values from the dataframe column 'year'\n",
    "years = df['year'].unique()\n",
    "\n",
    "# Convert'year' column to string data type and add it as a new dataframe column.\n",
    "df['year_str'] = df['year'].astype(str)\n",
    "\n",
    "# Keep only last two digits of year\n",
    "df['year_str'] = df['year_str'].str[-2:]\n",
    "\n",
    "# Generate indicator variable that takes value 1 if the last two digits of year is 13 or 14 and 0 otherwise.\n",
    "indicator_13_14 = df['year_str'].isin(['13','14'])\n",
    "\n",
    "# Extract observations corresponding to years 2013 and 2014\n",
    "df_13_14 = df[indicator_13_14]\n",
    "\n",
    "# Generate indicator variable that takes value 1 if 'year' is equal to 2013 or 2014 and value 0 otherwise.\n",
    "new_indicator_13_14 = (df['year']==2013)|(df['year']==2014)\n",
    "\n",
    "# Check if the previous indicator variable and the new indicator variable are equal\n",
    "check_equal = (indicator_13_14)==(new_indicator_13_14)\n",
    "# Each row of the above variable takes value 1 if the corresponding rows of the two indicators are equal and takes\n",
    "# value zero otherwise. We want to ensure that each row of check_equal has value 1. That is, the sum of check_euqal\n",
    "# is equal to the total number of observations.\n",
    "print(sum(check_equal)==len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 observations from 2013 Dataset\n",
      "   index  gvkey  year             Y             K       L             M  \\\n",
      "0     46   1050  2013    184.918233     16.236777   0.783    116.049983   \n",
      "1     70   1072  2013   1585.233591   1971.243363  10.800    577.475328   \n",
      "2     84   1078  2013  18269.697122  18833.963822  69.000  11070.662118   \n",
      "\n",
      "            VA            i  naics_4digit year_str  \n",
      "0    68.868250     1.296683          3334       13  \n",
      "1  1007.758263    46.177310          3344       13  \n",
      "2  7199.035004  1062.004359          3254       13  \n",
      "\n",
      "\n",
      "First 3 observations from 2014 Dataset\n",
      "   index  gvkey  year             Y             K       L            M  \\\n",
      "0     47   1050  2014    242.782036     34.842984   0.853   160.397790   \n",
      "1     71   1072  2014   1444.400192   2033.063657  10.700   558.888923   \n",
      "2     85   1078  2014  16812.535291  12870.000000  77.000  9070.023583   \n",
      "\n",
      "            VA           i  naics_4digit year_str  \n",
      "0    82.384246    1.065701          3334       14  \n",
      "1   885.511269   40.342829          3344       14  \n",
      "2  7742.511708  977.003674          3254       14  \n",
      "\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Construct indicator variable that takes value 1 if 'year' is equal to 2013 and 0 otherwise.\n",
    "indicator_13 = (df['year']==2013)\n",
    "\n",
    "# Construct dataset corresponding to year 2013\n",
    "df_13 = df[indicator_13]\n",
    "\n",
    "# Construct indicator variable that takes value 1 if 'year' is equal to 2014 and 0 otherwise.\n",
    "indicator_14 = (df['year']==2014)\n",
    "\n",
    "# Construct dataset corresponding to year 2014\n",
    "df_14 = df[indicator_14]\n",
    "\n",
    "# Extact distinct firm ids from 2013\n",
    "firms_13 = df_13['gvkey'].unique()\n",
    "# Convert to set object\n",
    "firms_13 = set(firms_13)\n",
    "\n",
    "# Extract distinct firm ids form 2013\n",
    "firms_14 = df_14['gvkey'].unique()\n",
    "# Conver to set object\n",
    "firms_14 = set(firms_14)\n",
    "\n",
    "# Sort each by firm id to ensure that the data rows are matched by firm id\n",
    "df_13.sort_values(by=['gvkey'])\n",
    "df_14.sort_values(by=['gvkey'])\n",
    "\n",
    "df_13 = df_13[df_13['gvkey'].isin(df_14['gvkey'])]\n",
    "df_14 = df_14[df_14['gvkey'].isin(df_13['gvkey'])]\n",
    "\n",
    "# Reset row indices\n",
    "df_13 = df_13.reset_index()\n",
    "df_14 = df_14.reset_index()\n",
    "\n",
    "\n",
    "print('First 3 observations from 2013 Dataset')\n",
    "print(df_13.head(3))\n",
    "print('\\n')\n",
    "print('First 3 observations from 2014 Dataset')\n",
    "print(df_14.head(3))\n",
    "print('\\n')\n",
    "print(len(df_13)==len(df_14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 29836 firm-year observations.\n",
      "\n",
      "\n",
      "How many distinct firms?\n",
      "There are 1270 firms that appear in both 2013 and 2014.\n",
      "\n",
      "\n",
      "Aggregate total sales across all manufacturing firms in 2014\n",
      "4598885.16562\n",
      "million USD\n",
      "\n",
      "\n",
      "How many workers did these firms employ in total?\n",
      "10561921.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1385.000000</td>\n",
       "      <td>1385.000000</td>\n",
       "      <td>1385.000000</td>\n",
       "      <td>1385.000000</td>\n",
       "      <td>1385.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3320.494704</td>\n",
       "      <td>2231.672204</td>\n",
       "      <td>7625.935740</td>\n",
       "      <td>2322.643726</td>\n",
       "      <td>177.861325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13757.130624</td>\n",
       "      <td>17611.446735</td>\n",
       "      <td>21993.220681</td>\n",
       "      <td>10755.259635</td>\n",
       "      <td>1339.764645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.015777</td>\n",
       "      <td>0.043616</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.076002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5%</th>\n",
       "      <td>2.381021</td>\n",
       "      <td>1.307091</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>6.797117</td>\n",
       "      <td>0.064936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.397880</td>\n",
       "      <td>14.930594</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>37.964972</td>\n",
       "      <td>1.025990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>335.954781</td>\n",
       "      <td>125.984336</td>\n",
       "      <td>994.000000</td>\n",
       "      <td>211.207990</td>\n",
       "      <td>10.953641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1824.290514</td>\n",
       "      <td>856.879813</td>\n",
       "      <td>5607.000000</td>\n",
       "      <td>1162.113794</td>\n",
       "      <td>63.954279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>13011.277576</td>\n",
       "      <td>6869.959647</td>\n",
       "      <td>34968.000000</td>\n",
       "      <td>8127.425641</td>\n",
       "      <td>542.736655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>259274.554682</td>\n",
       "      <td>525470.728374</td>\n",
       "      <td>302000.000000</td>\n",
       "      <td>219383.525866</td>\n",
       "      <td>32894.516806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Y              K              L              M  \\\n",
       "count    1385.000000    1385.000000    1385.000000    1385.000000   \n",
       "mean     3320.494704    2231.672204    7625.935740    2322.643726   \n",
       "std     13757.130624   17611.446735   21993.220681   10755.259635   \n",
       "min         0.015777       0.043616      11.000000       0.076002   \n",
       "5%          2.381021       1.307091      31.000000       6.797117   \n",
       "25%        39.397880      14.930594     152.000000      37.964972   \n",
       "50%       335.954781     125.984336     994.000000     211.207990   \n",
       "75%      1824.290514     856.879813    5607.000000    1162.113794   \n",
       "95%     13011.277576    6869.959647   34968.000000    8127.425641   \n",
       "max    259274.554682  525470.728374  302000.000000  219383.525866   \n",
       "\n",
       "                  i  \n",
       "count   1385.000000  \n",
       "mean     177.861325  \n",
       "std     1339.764645  \n",
       "min        0.000000  \n",
       "5%         0.064936  \n",
       "25%        1.025990  \n",
       "50%       10.953641  \n",
       "75%       63.954279  \n",
       "95%      542.736655  \n",
       "max    32894.516806  "
      ]
     },
     "execution_count": 914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.1\n",
    "firm_year_obs = 29836 #firm_year_obs\n",
    "print(\"We have %d firm-year observations.\" %firm_year_obs)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"How many distinct firms?\")\n",
    "# Obtain firms ids that appear in both 2013 and 2014. This is the intersection of the two sets above.\n",
    "firms_13_14 = firms_14.intersection(firms_13)\n",
    "print('There are %d firms that appear in both 2013 and 2014.' %len(firms_13_14))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "df14 = df[df['year']==2014]\n",
    "\n",
    "print(\"Aggregate total sales across all manufacturing firms in 2014\")\n",
    "numsum = sum(df14.Y)\n",
    "print(numsum) \n",
    "print('million USD')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "numsum1 = sum(df14.L)\n",
    "print(\"How many workers did these firms employ in total?\")\n",
    "print(1000*numsum1)\n",
    "\n",
    "#Disply information in a Table\n",
    "df14['L'] = df14['L']*1000\n",
    "\n",
    "df14[['Y','K','L','M','i']].describe(percentiles=[0.05, 0.25, 0.50, 0.75, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1Q: Write a few sentences summarizing dataset.\n",
    "\n",
    "A: This table shows descriptive statistics of total sales, capital stock, employees, materials, and investment across all manufacturing firms in 2014 in my dataset. For each variable, the mean is greater than the median, so each variable has a right-skewed distribution, so each variable has an outlier on the far right side, as can be seen in the maximum value of each variable, and also in how much they deviate from 95% value. This might suggest that the manufacturing industry heavily depends on the \"outlier\" large firms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/davidson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/davidson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/davidson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>logY14</td>      <th>  R-squared:         </th> <td>   0.911</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.911</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3987.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 08 Mar 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:14:20</td>     <th>  Log-Likelihood:    </th> <td> -1659.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1385</td>      <th>  AIC:               </th> <td>   3326.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1381</td>      <th>  BIC:               </th> <td>   3347.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC0</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>   -1.3562</td> <td>    0.111</td> <td>  -12.206</td> <td> 0.000</td> <td>   -1.574</td> <td>   -1.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logK14</th> <td>    0.1589</td> <td>    0.029</td> <td>    5.504</td> <td> 0.000</td> <td>    0.102</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logL14</th> <td>    0.4859</td> <td>    0.031</td> <td>   15.575</td> <td> 0.000</td> <td>    0.425</td> <td>    0.547</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logM14</th> <td>    0.5132</td> <td>    0.035</td> <td>   14.524</td> <td> 0.000</td> <td>    0.444</td> <td>    0.582</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>888.611</td> <th>  Durbin-Watson:     </th> <td>   2.018</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>11588.718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.817</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.003</td>  <th>  Cond. No.          </th> <td>    47.4</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 logY14   R-squared:                       0.911\n",
       "Model:                            OLS   Adj. R-squared:                  0.911\n",
       "Method:                 Least Squares   F-statistic:                     3987.\n",
       "Date:                Thu, 08 Mar 2018   Prob (F-statistic):               0.00\n",
       "Time:                        17:14:20   Log-Likelihood:                -1659.1\n",
       "No. Observations:                1385   AIC:                             3326.\n",
       "Df Residuals:                    1381   BIC:                             3347.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:                  HC0                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -1.3562      0.111    -12.206      0.000      -1.574      -1.138\n",
       "logK14         0.1589      0.029      5.504      0.000       0.102       0.215\n",
       "logL14         0.4859      0.031     15.575      0.000       0.425       0.547\n",
       "logM14         0.5132      0.035     14.524      0.000       0.444       0.582\n",
       "==============================================================================\n",
       "Omnibus:                      888.611   Durbin-Watson:                   2.018\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            11588.718\n",
       "Skew:                          -2.817   Prob(JB):                         0.00\n",
       "Kurtosis:                      16.003   Cond. No.                         47.4\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
       "\"\"\""
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.2 OLS production function estimates\n",
    "\n",
    "#same df1 from above--for 2014 only\n",
    "df14['logY14'] =np.log(df14[\"Y\"])\n",
    "df14['logK14'] =np.log(df14[\"K\"])\n",
    "df14['logL14'] =np.log(df14[\"L\"])\n",
    "df14['logM14'] =np.log(df14[\"M\"])\n",
    "\n",
    "Y = df14[\"logY14\"]\n",
    "X = df14[[\"logK14\",\"logL14\",\"logM14\"]]\n",
    "X = sm.add_constant(X) #Add aconstant term to the predictor\n",
    "# OLS regression\n",
    "ols = sm.OLS(Y,X).fit(cov_type='HC0') #Heteroscedastic robust standard errors\n",
    "ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const    -1.356232\n",
      "logK14    0.158893\n",
      "logL14    0.485932\n",
      "logM14    0.513235\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "           const    logK14    logL14    logM14\n",
      "const   0.012347  0.000759 -0.002776  0.000900\n",
      "logK14  0.000759  0.000833 -0.000200 -0.000656\n",
      "logL14 -0.002776 -0.000200  0.000973 -0.000608\n",
      "logM14  0.000900 -0.000656 -0.000608  0.001249\n",
      "\n",
      "\n",
      "14.0297113153\n",
      "                             Test for Constraints                             \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "c0             1.1581      0.011     14.030      0.000       1.136       1.180\n",
      "==============================================================================\n",
      "we can reject the null hypothesis under 95% significance level.\n"
     ]
    }
   ],
   "source": [
    "#Coefficient vector and Covariance Matrix\n",
    "coef1 = ols.params\n",
    "cov1 = ols.cov_params()\n",
    "\n",
    "#var = np.diag(cov1) # Obtain diagonal elements of cov\n",
    "\n",
    "print (coef1)\n",
    "print ('\\n')\n",
    "print (cov1)\n",
    "print ('\\n')\n",
    "\n",
    "#Hyptohesis Testing\n",
    "t = (sum(ols.params[1:4])-1)/(sum(np.diag(cov1))-cov1['const'][0]+2*(cov1['logL14'][1]+cov1['logM14'][1]+cov1['logM14'][2]))**(0.5)\n",
    "print(t)\n",
    "hypotheses = 'logK14+logL14+logM14=1'\n",
    "t_test = ols.t_test(hypotheses)\n",
    "print(t_test)\n",
    "print(\"we can reject the null hypothesis under 95% significance level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2Q: Briefly Comment on your results. Can you reject the null hpyothesis on constant returns to scale? Do you believe your coefficient estimates are consistent for the underlying output elasticities? Why?\n",
    "\n",
    "A: When the product function is given as $Y=AK^{\\alpha}L^{\\beta}M^{\\gamma}$, we are estimating the elasticity of output with respect to capital, labor, and material expenditures using a simple OLS regression. When we take the log of each side of the function, then we have the following expression: $log(Y)=log(A)+\\alpha log(K)+\\beta log(L)+\\gamma log(M)$ = $E[log(A)]+log(A)+\\alpha log(K)+\\beta log(L)+\\gamma log(M)+\\epsilon$ , where A is the unobservable productivity of the firm, and the error is  $\\epsilon = log(A)-E[log(A)]$.\n",
    "Then, our estimating function will have this OLS estimation of projection of form $E^{*}[log(Y)|log(K),log(L),log(M)]$, and our coefficients will be the estimates for elasticities of output in the sample. The sum of $\\hat{\\alpha}$,$\\hat{\\beta}$, and $\\hat{\\gamma}$ will give us information about returns to scale. if the sum is equal to one we will have constant returns to scale.\n",
    "Our estimated coefficients for the elasticity of output with respect to capital, labor, and material expenditures are 0.1589, 0.4859, and 0.5132, respectively. Here, $\\alpha = 0.1589$ means 0.1589% changes in Y is induced by 1% change in K, holding other variables constant. Under the null hypothesis of ${\\alpha}+{\\beta}+{\\gamma}=1$, we get a test statistic approximately equal to 14.03, much higher than 1.96 of 95% confidence interval. Thus, we cannot reject the null hyptoehsis of constant returns to scale under this significance level 0.05.\n",
    "The error term here $\\epsilon = log(A)-E[log(A)]$ has a mean of zero and is likely to lead to unbiased estimators for the underlying output elasticities. However, we still have to ask if there is endogeneity bias that might happen when regressor variables are correlated with error terms. But it is doubtful that each input demand of log capital, log labor, and log materials is uncorrleated with the error term $\\epsilon = log(A)-E[log(A)]$, or simply with log productivity. Thus, It might be hard to believe that the coefficient estimates are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>logY14</td>      <th>  R-squared:         </th> <td>   0.922</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.917</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>8.679e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 08 Mar 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:14:20</td>     <th>  Log-Likelihood:    </th> <td> -1571.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1385</td>      <th>  AIC:               </th> <td>   3307.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1303</td>      <th>  BIC:               </th> <td>   3737.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    81</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC0</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>   -1.2037</td> <td>    0.161</td> <td>   -7.490</td> <td> 0.000</td> <td>   -1.519</td> <td>   -0.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logK14</th>  <td>    0.1405</td> <td>    0.032</td> <td>    4.412</td> <td> 0.000</td> <td>    0.078</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logL14</th>  <td>    0.4528</td> <td>    0.035</td> <td>   12.986</td> <td> 0.000</td> <td>    0.384</td> <td>    0.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logM14</th>  <td>    0.5388</td> <td>    0.036</td> <td>   14.826</td> <td> 0.000</td> <td>    0.468</td> <td>    0.610</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3111</th> <td>   -0.0801</td> <td>    0.110</td> <td>   -0.731</td> <td> 0.465</td> <td>   -0.295</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3112</th> <td>    0.0167</td> <td>    0.157</td> <td>    0.106</td> <td> 0.916</td> <td>   -0.291</td> <td>    0.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3113</th> <td>    0.1755</td> <td>    0.196</td> <td>    0.897</td> <td> 0.370</td> <td>   -0.208</td> <td>    0.559</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3114</th> <td>    0.1271</td> <td>    0.136</td> <td>    0.936</td> <td> 0.349</td> <td>   -0.139</td> <td>    0.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3115</th> <td>    0.3641</td> <td>    0.193</td> <td>    1.889</td> <td> 0.059</td> <td>   -0.014</td> <td>    0.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3116</th> <td>   -0.1595</td> <td>    0.133</td> <td>   -1.202</td> <td> 0.229</td> <td>   -0.419</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3118</th> <td>   -0.0988</td> <td>    0.114</td> <td>   -0.869</td> <td> 0.385</td> <td>   -0.322</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3119</th> <td>    0.0185</td> <td>    0.167</td> <td>    0.110</td> <td> 0.912</td> <td>   -0.310</td> <td>    0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3121</th> <td>    0.3076</td> <td>    0.137</td> <td>    2.248</td> <td> 0.025</td> <td>    0.039</td> <td>    0.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3122</th> <td>    0.4875</td> <td>    0.182</td> <td>    2.682</td> <td> 0.007</td> <td>    0.131</td> <td>    0.844</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3131</th> <td>   -0.1640</td> <td>    0.121</td> <td>   -1.356</td> <td> 0.175</td> <td>   -0.401</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3132</th> <td>    0.2061</td> <td>    0.231</td> <td>    0.893</td> <td> 0.372</td> <td>   -0.246</td> <td>    0.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3141</th> <td>   -0.0107</td> <td>    0.163</td> <td>   -0.065</td> <td> 0.948</td> <td>   -0.331</td> <td>    0.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3151</th> <td>    0.1335</td> <td>    0.126</td> <td>    1.059</td> <td> 0.289</td> <td>   -0.114</td> <td>    0.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3152</th> <td>   -0.0324</td> <td>    0.122</td> <td>   -0.266</td> <td> 0.790</td> <td>   -0.271</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3159</th> <td>    0.4428</td> <td>    0.118</td> <td>    3.751</td> <td> 0.000</td> <td>    0.211</td> <td>    0.674</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3162</th> <td>    0.0377</td> <td>    0.124</td> <td>    0.303</td> <td> 0.762</td> <td>   -0.206</td> <td>    0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3169</th> <td>    0.5013</td> <td>    0.313</td> <td>    1.602</td> <td> 0.109</td> <td>   -0.112</td> <td>    1.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3211</th> <td>   -0.3412</td> <td>    0.469</td> <td>   -0.728</td> <td> 0.467</td> <td>   -1.260</td> <td>    0.578</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3212</th> <td>    0.0671</td> <td>    0.153</td> <td>    0.439</td> <td> 0.660</td> <td>   -0.232</td> <td>    0.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3219</th> <td>    0.1345</td> <td>    0.124</td> <td>    1.083</td> <td> 0.279</td> <td>   -0.109</td> <td>    0.378</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3221</th> <td>   -0.0310</td> <td>    0.129</td> <td>   -0.240</td> <td> 0.811</td> <td>   -0.285</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3222</th> <td>   -0.1132</td> <td>    0.130</td> <td>   -0.868</td> <td> 0.385</td> <td>   -0.369</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3231</th> <td>    0.0114</td> <td>    0.130</td> <td>    0.088</td> <td> 0.930</td> <td>   -0.244</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3241</th> <td>    0.4693</td> <td>    0.157</td> <td>    2.988</td> <td> 0.003</td> <td>    0.162</td> <td>    0.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3251</th> <td>    0.1885</td> <td>    0.165</td> <td>    1.146</td> <td> 0.252</td> <td>   -0.134</td> <td>    0.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3252</th> <td>    0.0959</td> <td>    0.151</td> <td>    0.633</td> <td> 0.526</td> <td>   -0.201</td> <td>    0.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3253</th> <td>    0.2258</td> <td>    0.204</td> <td>    1.105</td> <td> 0.269</td> <td>   -0.175</td> <td>    0.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3254</th> <td>   -0.4723</td> <td>    0.135</td> <td>   -3.486</td> <td> 0.000</td> <td>   -0.738</td> <td>   -0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3255</th> <td>    0.0970</td> <td>    0.167</td> <td>    0.579</td> <td> 0.562</td> <td>   -0.231</td> <td>    0.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3256</th> <td>    0.2204</td> <td>    0.143</td> <td>    1.544</td> <td> 0.123</td> <td>   -0.059</td> <td>    0.500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3259</th> <td>   -0.1584</td> <td>    0.242</td> <td>   -0.656</td> <td> 0.512</td> <td>   -0.632</td> <td>    0.315</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3261</th> <td>    0.0857</td> <td>    0.139</td> <td>    0.617</td> <td> 0.538</td> <td>   -0.187</td> <td>    0.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3262</th> <td>    0.0333</td> <td>    0.188</td> <td>    0.177</td> <td> 0.859</td> <td>   -0.335</td> <td>    0.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3271</th> <td>    0.2824</td> <td>    0.139</td> <td>    2.035</td> <td> 0.042</td> <td>    0.010</td> <td>    0.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3272</th> <td>   -0.1743</td> <td>    0.126</td> <td>   -1.383</td> <td> 0.167</td> <td>   -0.421</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3273</th> <td>    0.0327</td> <td>    0.124</td> <td>    0.264</td> <td> 0.792</td> <td>   -0.210</td> <td>    0.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3274</th> <td>    0.1092</td> <td>    0.207</td> <td>    0.527</td> <td> 0.598</td> <td>   -0.297</td> <td>    0.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3279</th> <td>    0.1709</td> <td>    0.167</td> <td>    1.024</td> <td> 0.306</td> <td>   -0.156</td> <td>    0.498</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3311</th> <td>   -0.0063</td> <td>    0.133</td> <td>   -0.047</td> <td> 0.962</td> <td>   -0.267</td> <td>    0.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3312</th> <td>    0.1358</td> <td>    0.193</td> <td>    0.704</td> <td> 0.482</td> <td>   -0.243</td> <td>    0.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3313</th> <td>    0.0919</td> <td>    0.120</td> <td>    0.763</td> <td> 0.446</td> <td>   -0.144</td> <td>    0.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3314</th> <td>   -0.0172</td> <td>    0.200</td> <td>   -0.086</td> <td> 0.932</td> <td>   -0.409</td> <td>    0.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3321</th> <td>   -0.0259</td> <td>    0.112</td> <td>   -0.232</td> <td> 0.816</td> <td>   -0.245</td> <td>    0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3322</th> <td>    0.1096</td> <td>    0.131</td> <td>    0.834</td> <td> 0.404</td> <td>   -0.148</td> <td>    0.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3323</th> <td>    0.0782</td> <td>    0.135</td> <td>    0.577</td> <td> 0.564</td> <td>   -0.187</td> <td>    0.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3324</th> <td>    0.0643</td> <td>    0.156</td> <td>    0.411</td> <td> 0.681</td> <td>   -0.242</td> <td>    0.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3325</th> <td>    0.1425</td> <td>    0.115</td> <td>    1.238</td> <td> 0.216</td> <td>   -0.083</td> <td>    0.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3326</th> <td>    0.1572</td> <td>    0.138</td> <td>    1.143</td> <td> 0.253</td> <td>   -0.112</td> <td>    0.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3327</th> <td>    0.2691</td> <td>    0.168</td> <td>    1.601</td> <td> 0.109</td> <td>   -0.060</td> <td>    0.599</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3328</th> <td>    0.3150</td> <td>    0.120</td> <td>    2.619</td> <td> 0.009</td> <td>    0.079</td> <td>    0.551</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3329</th> <td>    0.0891</td> <td>    0.124</td> <td>    0.718</td> <td> 0.473</td> <td>   -0.154</td> <td>    0.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3331</th> <td>    0.0186</td> <td>    0.137</td> <td>    0.136</td> <td> 0.892</td> <td>   -0.249</td> <td>    0.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3332</th> <td>    0.0379</td> <td>    0.144</td> <td>    0.264</td> <td> 0.792</td> <td>   -0.243</td> <td>    0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3333</th> <td>    0.3761</td> <td>    0.130</td> <td>    2.899</td> <td> 0.004</td> <td>    0.122</td> <td>    0.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3334</th> <td>    0.0607</td> <td>    0.126</td> <td>    0.483</td> <td> 0.629</td> <td>   -0.186</td> <td>    0.307</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3335</th> <td>   -0.1165</td> <td>    0.134</td> <td>   -0.871</td> <td> 0.384</td> <td>   -0.379</td> <td>    0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3336</th> <td>   -0.0052</td> <td>    0.185</td> <td>   -0.028</td> <td> 0.978</td> <td>   -0.368</td> <td>    0.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3339</th> <td>    0.0326</td> <td>    0.158</td> <td>    0.206</td> <td> 0.837</td> <td>   -0.278</td> <td>    0.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3341</th> <td>    0.2793</td> <td>    0.122</td> <td>    2.288</td> <td> 0.022</td> <td>    0.040</td> <td>    0.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3342</th> <td>    0.3701</td> <td>    0.121</td> <td>    3.069</td> <td> 0.002</td> <td>    0.134</td> <td>    0.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3343</th> <td>    0.6486</td> <td>    0.249</td> <td>    2.609</td> <td> 0.009</td> <td>    0.161</td> <td>    1.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3344</th> <td>    0.1873</td> <td>    0.116</td> <td>    1.612</td> <td> 0.107</td> <td>   -0.040</td> <td>    0.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3345</th> <td>    0.2119</td> <td>    0.118</td> <td>    1.788</td> <td> 0.074</td> <td>   -0.020</td> <td>    0.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3346</th> <td>    0.4457</td> <td>    0.118</td> <td>    3.764</td> <td> 0.000</td> <td>    0.214</td> <td>    0.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3351</th> <td>    0.5500</td> <td>    0.189</td> <td>    2.904</td> <td> 0.004</td> <td>    0.179</td> <td>    0.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3352</th> <td>   -0.2483</td> <td>    0.147</td> <td>   -1.693</td> <td> 0.090</td> <td>   -0.536</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3353</th> <td>    0.0545</td> <td>    0.144</td> <td>    0.378</td> <td> 0.706</td> <td>   -0.228</td> <td>    0.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3359</th> <td>   -0.0681</td> <td>    0.148</td> <td>   -0.459</td> <td> 0.646</td> <td>   -0.359</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3361</th> <td>   -0.1298</td> <td>    0.131</td> <td>   -0.990</td> <td> 0.322</td> <td>   -0.387</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3362</th> <td>    0.1149</td> <td>    0.125</td> <td>    0.923</td> <td> 0.356</td> <td>   -0.129</td> <td>    0.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3363</th> <td>    0.0515</td> <td>    0.119</td> <td>    0.432</td> <td> 0.666</td> <td>   -0.182</td> <td>    0.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3364</th> <td>   -0.0203</td> <td>    0.122</td> <td>   -0.166</td> <td> 0.868</td> <td>   -0.260</td> <td>    0.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3365</th> <td>    0.0071</td> <td>    0.136</td> <td>    0.052</td> <td> 0.958</td> <td>   -0.259</td> <td>    0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3366</th> <td>    0.2721</td> <td>    0.273</td> <td>    0.997</td> <td> 0.319</td> <td>   -0.263</td> <td>    0.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3369</th> <td>    0.3325</td> <td>    0.120</td> <td>    2.767</td> <td> 0.006</td> <td>    0.097</td> <td>    0.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3371</th> <td>    0.0605</td> <td>    0.126</td> <td>    0.479</td> <td> 0.632</td> <td>   -0.187</td> <td>    0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3372</th> <td>   -0.1250</td> <td>    0.119</td> <td>   -1.055</td> <td> 0.291</td> <td>   -0.357</td> <td>    0.107</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>788.585</td> <th>  Durbin-Watson:     </th> <td>   2.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>9666.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.408</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>15.013</td>  <th>  Cond. No.          </th> <td>2.05e+03</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 logY14   R-squared:                       0.922\n",
       "Model:                            OLS   Adj. R-squared:                  0.917\n",
       "Method:                 Least Squares   F-statistic:                 8.679e+15\n",
       "Date:                Thu, 08 Mar 2018   Prob (F-statistic):               0.00\n",
       "Time:                        17:14:20   Log-Likelihood:                -1571.7\n",
       "No. Observations:                1385   AIC:                             3307.\n",
       "Df Residuals:                    1303   BIC:                             3737.\n",
       "Df Model:                          81                                         \n",
       "Covariance Type:                  HC0                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -1.2037      0.161     -7.490      0.000      -1.519      -0.889\n",
       "logK14         0.1405      0.032      4.412      0.000       0.078       0.203\n",
       "logL14         0.4528      0.035     12.986      0.000       0.384       0.521\n",
       "logM14         0.5388      0.036     14.826      0.000       0.468       0.610\n",
       "n4_3111       -0.0801      0.110     -0.731      0.465      -0.295       0.135\n",
       "n4_3112        0.0167      0.157      0.106      0.916      -0.291       0.325\n",
       "n4_3113        0.1755      0.196      0.897      0.370      -0.208       0.559\n",
       "n4_3114        0.1271      0.136      0.936      0.349      -0.139       0.393\n",
       "n4_3115        0.3641      0.193      1.889      0.059      -0.014       0.742\n",
       "n4_3116       -0.1595      0.133     -1.202      0.229      -0.419       0.100\n",
       "n4_3118       -0.0988      0.114     -0.869      0.385      -0.322       0.124\n",
       "n4_3119        0.0185      0.167      0.110      0.912      -0.310       0.347\n",
       "n4_3121        0.3076      0.137      2.248      0.025       0.039       0.576\n",
       "n4_3122        0.4875      0.182      2.682      0.007       0.131       0.844\n",
       "n4_3131       -0.1640      0.121     -1.356      0.175      -0.401       0.073\n",
       "n4_3132        0.2061      0.231      0.893      0.372      -0.246       0.659\n",
       "n4_3141       -0.0107      0.163     -0.065      0.948      -0.331       0.309\n",
       "n4_3151        0.1335      0.126      1.059      0.289      -0.114       0.380\n",
       "n4_3152       -0.0324      0.122     -0.266      0.790      -0.271       0.206\n",
       "n4_3159        0.4428      0.118      3.751      0.000       0.211       0.674\n",
       "n4_3162        0.0377      0.124      0.303      0.762      -0.206       0.281\n",
       "n4_3169        0.5013      0.313      1.602      0.109      -0.112       1.114\n",
       "n4_3211       -0.3412      0.469     -0.728      0.467      -1.260       0.578\n",
       "n4_3212        0.0671      0.153      0.439      0.660      -0.232       0.366\n",
       "n4_3219        0.1345      0.124      1.083      0.279      -0.109       0.378\n",
       "n4_3221       -0.0310      0.129     -0.240      0.811      -0.285       0.223\n",
       "n4_3222       -0.1132      0.130     -0.868      0.385      -0.369       0.142\n",
       "n4_3231        0.0114      0.130      0.088      0.930      -0.244       0.267\n",
       "n4_3241        0.4693      0.157      2.988      0.003       0.162       0.777\n",
       "n4_3251        0.1885      0.165      1.146      0.252      -0.134       0.511\n",
       "n4_3252        0.0959      0.151      0.633      0.526      -0.201       0.393\n",
       "n4_3253        0.2258      0.204      1.105      0.269      -0.175       0.626\n",
       "n4_3254       -0.4723      0.135     -3.486      0.000      -0.738      -0.207\n",
       "n4_3255        0.0970      0.167      0.579      0.562      -0.231       0.425\n",
       "n4_3256        0.2204      0.143      1.544      0.123      -0.059       0.500\n",
       "n4_3259       -0.1584      0.242     -0.656      0.512      -0.632       0.315\n",
       "n4_3261        0.0857      0.139      0.617      0.538      -0.187       0.358\n",
       "n4_3262        0.0333      0.188      0.177      0.859      -0.335       0.402\n",
       "n4_3271        0.2824      0.139      2.035      0.042       0.010       0.554\n",
       "n4_3272       -0.1743      0.126     -1.383      0.167      -0.421       0.073\n",
       "n4_3273        0.0327      0.124      0.264      0.792      -0.210       0.275\n",
       "n4_3274        0.1092      0.207      0.527      0.598      -0.297       0.515\n",
       "n4_3279        0.1709      0.167      1.024      0.306      -0.156       0.498\n",
       "n4_3311       -0.0063      0.133     -0.047      0.962      -0.267       0.254\n",
       "n4_3312        0.1358      0.193      0.704      0.482      -0.243       0.514\n",
       "n4_3313        0.0919      0.120      0.763      0.446      -0.144       0.328\n",
       "n4_3314       -0.0172      0.200     -0.086      0.932      -0.409       0.374\n",
       "n4_3321       -0.0259      0.112     -0.232      0.816      -0.245       0.193\n",
       "n4_3322        0.1096      0.131      0.834      0.404      -0.148       0.367\n",
       "n4_3323        0.0782      0.135      0.577      0.564      -0.187       0.344\n",
       "n4_3324        0.0643      0.156      0.411      0.681      -0.242       0.371\n",
       "n4_3325        0.1425      0.115      1.238      0.216      -0.083       0.368\n",
       "n4_3326        0.1572      0.138      1.143      0.253      -0.112       0.427\n",
       "n4_3327        0.2691      0.168      1.601      0.109      -0.060       0.599\n",
       "n4_3328        0.3150      0.120      2.619      0.009       0.079       0.551\n",
       "n4_3329        0.0891      0.124      0.718      0.473      -0.154       0.332\n",
       "n4_3331        0.0186      0.137      0.136      0.892      -0.249       0.286\n",
       "n4_3332        0.0379      0.144      0.264      0.792      -0.243       0.319\n",
       "n4_3333        0.3761      0.130      2.899      0.004       0.122       0.630\n",
       "n4_3334        0.0607      0.126      0.483      0.629      -0.186       0.307\n",
       "n4_3335       -0.1165      0.134     -0.871      0.384      -0.379       0.146\n",
       "n4_3336       -0.0052      0.185     -0.028      0.978      -0.368       0.358\n",
       "n4_3339        0.0326      0.158      0.206      0.837      -0.278       0.343\n",
       "n4_3341        0.2793      0.122      2.288      0.022       0.040       0.519\n",
       "n4_3342        0.3701      0.121      3.069      0.002       0.134       0.606\n",
       "n4_3343        0.6486      0.249      2.609      0.009       0.161       1.136\n",
       "n4_3344        0.1873      0.116      1.612      0.107      -0.040       0.415\n",
       "n4_3345        0.2119      0.118      1.788      0.074      -0.020       0.444\n",
       "n4_3346        0.4457      0.118      3.764      0.000       0.214       0.678\n",
       "n4_3351        0.5500      0.189      2.904      0.004       0.179       0.921\n",
       "n4_3352       -0.2483      0.147     -1.693      0.090      -0.536       0.039\n",
       "n4_3353        0.0545      0.144      0.378      0.706      -0.228       0.337\n",
       "n4_3359       -0.0681      0.148     -0.459      0.646      -0.359       0.223\n",
       "n4_3361       -0.1298      0.131     -0.990      0.322      -0.387       0.127\n",
       "n4_3362        0.1149      0.125      0.923      0.356      -0.129       0.359\n",
       "n4_3363        0.0515      0.119      0.432      0.666      -0.182       0.285\n",
       "n4_3364       -0.0203      0.122     -0.166      0.868      -0.260       0.219\n",
       "n4_3365        0.0071      0.136      0.052      0.958      -0.259       0.273\n",
       "n4_3366        0.2721      0.273      0.997      0.319      -0.263       0.807\n",
       "n4_3369        0.3325      0.120      2.767      0.006       0.097       0.568\n",
       "n4_3371        0.0605      0.126      0.479      0.632      -0.187       0.308\n",
       "n4_3372       -0.1250      0.119     -1.055      0.291      -0.357       0.107\n",
       "==============================================================================\n",
       "Omnibus:                      788.585   Durbin-Watson:                   2.075\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             9666.149\n",
       "Skew:                          -2.408   Prob(JB):                         0.00\n",
       "Kurtosis:                      15.013   Cond. No.                     2.05e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
       "[2] The condition number is large, 2.05e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 917,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.2 OLS with dummy variables\n",
    "\n",
    "Y = df14[\"logY14\"]\n",
    "X = df14[[\"logK14\",\"logL14\",\"logM14\"]]\n",
    "X = sm.add_constant(X) #Add aconstant term to the predictor\n",
    "\n",
    "# Construct a list of all naics_4digit in the dataset\n",
    "included_4digit = sorted (df14['naics_4digit'].unique ())\n",
    "# Form dummies for each of naics_4digit\n",
    "pair_dums = pd.get_dummies(df14['naics_4digit'].astype(\"category\"), prefix=\"n4\")\n",
    "# Concatenate matched pair dummies onto dataframe \n",
    "df14 = pd. concat ([ df14 , pair_dums ] , axis=1)\n",
    "\n",
    "X = pd.concat([X, pair_dums.iloc[:,0:-1]], axis=1)\n",
    "#OLS regression with dummy variables\n",
    "ols2 = sm.OLS(Y,X).fit(cov_type=\"HC0\")\n",
    "ols2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Test for Constraints                             \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "c0             1.1321      0.011     12.563      0.000       1.111       1.153\n",
      "==============================================================================\n",
      "we can reject the null hypothesis under 95% significance level.\n"
     ]
    }
   ],
   "source": [
    "#Coefficient vector and Covariance Matrix\n",
    "coef2= ols2.params\n",
    "cov2 = ols2.cov_params()\n",
    "\n",
    "#Hyptohesis Testing\n",
    "t = (sum(ols2.params[1:4])-1)/np.sqrt(cov2['logK14'][1]+cov2['logL14'][2]+cov2['logM14'][3]+2*(cov2['logL14'][1]+cov2['logM14'][1]+cov2['logM14'][2]))\n",
    "#12.56\n",
    "#sum(ols2.params[1:4])-1\n",
    "t\n",
    "hypotheses = 'logK14+logL14+logM14=1'\n",
    "t_test = ols2.t_test(hypotheses)\n",
    "print(t_test)\n",
    "print(\"we can reject the null hypothesis under 95% significance level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2Q: Briefly comment on your results. Can you reject the null hypothesis of constant returns to scale? Do you believe your coefficient estimates are consistent for the underlying ouptut elasticities? Why?\n",
    "\n",
    "A: For the OLS with dummy variables included, our estimated coefficients for the elasticity of output with respect to capital ($\\alpha$), labor ($\\beta$), and material expenditures ($\\gamma$) are 0.1405, 0.4528, and 0.5338, respectively.\n",
    "Here, $\\alpha = 0.1405$ means 0.1405% changes in Y is induced by 1% change in K, holding other variables constant.\n",
    "By adding dummy variables specific to industry segment to the regression, we are fixing variables specific to the industry sector, possilby removing omitted variable bias. Under the null hypothesis of ${\\alpha}+{\\beta}+{\\gamma}=1$, we get a test statistic approximately equal to 12.56, higher than 1.96 of 95% confidence interval. Thus, we can reject the null hyptoehsis of constant returns to scale at this significance level.\n",
    "However, we still have to ask if there is endogeneity bias that might happen when regressor variables are correlated with error terms. Other than fixing variables that might bias the coefficients for the elasticity of output,it is still doubtful that each input demand of log capital, log labor, and log materials is uncorrleated with the error term, or with log productivity. Thus, adding dummies to simple OLS regression might not be enough to make the coefficient estimates consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>deltalogY</td>    <th>  R-squared:         </th> <td>   0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   25.58</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 08 Mar 2018</td> <th>  Prob (F-statistic):</th> <td>4.55e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:14:20</td>     <th>  Log-Likelihood:    </th> <td> -654.15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1270</td>      <th>  AIC:               </th> <td>   1316.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1266</td>      <th>  BIC:               </th> <td>   1337.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC0</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th>  <td>    0.0270</td> <td>    0.010</td> <td>    2.657</td> <td> 0.008</td> <td>    0.007</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deltalogK</th> <td>   -0.0035</td> <td>    0.054</td> <td>   -0.065</td> <td> 0.948</td> <td>   -0.110</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deltalogL</th> <td>    0.4212</td> <td>    0.134</td> <td>    3.148</td> <td> 0.002</td> <td>    0.159</td> <td>    0.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deltalogM</th> <td>    0.5185</td> <td>    0.074</td> <td>    7.018</td> <td> 0.000</td> <td>    0.374</td> <td>    0.663</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1393.059</td> <th>  Durbin-Watson:     </th>  <td>   2.044</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>289316.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 4.949</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>76.276</td>  <th>  Cond. No.          </th>  <td>    5.01</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              deltalogY   R-squared:                       0.202\n",
       "Model:                            OLS   Adj. R-squared:                  0.200\n",
       "Method:                 Least Squares   F-statistic:                     25.58\n",
       "Date:                Thu, 08 Mar 2018   Prob (F-statistic):           4.55e-16\n",
       "Time:                        17:14:20   Log-Likelihood:                -654.15\n",
       "No. Observations:                1270   AIC:                             1316.\n",
       "Df Residuals:                    1266   BIC:                             1337.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:                  HC0                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "constant       0.0270      0.010      2.657      0.008       0.007       0.047\n",
       "deltalogK     -0.0035      0.054     -0.065      0.948      -0.110       0.103\n",
       "deltalogL      0.4212      0.134      3.148      0.002       0.159       0.683\n",
       "deltalogM      0.5185      0.074      7.018      0.000       0.374       0.663\n",
       "==============================================================================\n",
       "Omnibus:                     1393.059   Durbin-Watson:                   2.044\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           289316.900\n",
       "Skew:                           4.949   Prob(JB):                         0.00\n",
       "Kurtosis:                      76.276   Cond. No.                         5.01\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
       "\"\"\""
      ]
     },
     "execution_count": 919,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.3 Panel Production Function Estimates\n",
    "\n",
    "df_13['logY13'] = np.log(df_13['Y'])\n",
    "df_13['logK13'] = np.log(df_13['K'])\n",
    "df_13['logL13'] =np.log(df_13['L'])\n",
    "df_13['logM13'] =np.log(df_13['M'])\n",
    "\n",
    "df_14['logY14'] = np.log(df_14['Y'])\n",
    "df_14['logK14'] = np.log(df_14['K'])\n",
    "df_14['logL14'] = np.log(df_14['L'])\n",
    "df_14['logM14'] = np.log(df_14['M'])\n",
    "\n",
    "df_13['constant'] =1\n",
    "df_13['deltalogY'] = df_14.logY14-df_13.logY13\n",
    "df_13['deltalogK'] = df_14.logK14-df_13.logK13\n",
    "df_13['deltalogL'] = df_14.logL14-df_13.logL13\n",
    "df_13['deltalogM'] = df_14.logM14-df_13.logM13\n",
    "X1=df_13[['constant','deltalogK','deltalogL','deltalogM']]\n",
    "#Panel Data Regression\n",
    "ols3=sm.OLS(df_13.deltalogY,X1).fit(cov_type=\"HC0\")\n",
    "ols3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/davidson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/davidson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/davidson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/davidson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>logY1314</td>     <th>  R-squared:         </th> <td>   0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   25.58</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 08 Mar 2018</td> <th>  Prob (F-statistic):</th> <td>4.55e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:14:24</td>     <th>  Log-Likelihood:    </th> <td> -654.15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1270</td>      <th>  AIC:               </th> <td>   1316.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1266</td>      <th>  BIC:               </th> <td>   1337.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC0</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th> <td>    0.0270</td> <td>    0.010</td> <td>    2.657</td> <td> 0.008</td> <td>    0.007</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logK1314</th> <td>   -0.0035</td> <td>    0.054</td> <td>   -0.065</td> <td> 0.948</td> <td>   -0.110</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logL1314</th> <td>    0.4212</td> <td>    0.134</td> <td>    3.148</td> <td> 0.002</td> <td>    0.159</td> <td>    0.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logM1314</th> <td>    0.5185</td> <td>    0.074</td> <td>    7.018</td> <td> 0.000</td> <td>    0.374</td> <td>    0.663</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1393.059</td> <th>  Durbin-Watson:     </th>  <td>   2.044</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>289316.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 4.949</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>76.276</td>  <th>  Cond. No.          </th>  <td>    5.01</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               logY1314   R-squared:                       0.202\n",
       "Model:                            OLS   Adj. R-squared:                  0.200\n",
       "Method:                 Least Squares   F-statistic:                     25.58\n",
       "Date:                Thu, 08 Mar 2018   Prob (F-statistic):           4.55e-16\n",
       "Time:                        17:14:24   Log-Likelihood:                -654.15\n",
       "No. Observations:                1270   AIC:                             1316.\n",
       "Df Residuals:                    1266   BIC:                             1337.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:                  HC0                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "constant       0.0270      0.010      2.657      0.008       0.007       0.047\n",
       "logK1314      -0.0035      0.054     -0.065      0.948      -0.110       0.103\n",
       "logL1314       0.4212      0.134      3.148      0.002       0.159       0.683\n",
       "logM1314       0.5185      0.074      7.018      0.000       0.374       0.663\n",
       "==============================================================================\n",
       "Omnibus:                     1393.059   Durbin-Watson:                   2.044\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           289316.900\n",
       "Skew:                           4.949   Prob(JB):                         0.00\n",
       "Kurtosis:                      76.276   Cond. No.                         5.01\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
       "\"\"\""
      ]
     },
     "execution_count": 920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.3 Panel production function estimates -- another version\n",
    "\n",
    "df2 = df[df[\"year\"]>=2013]\n",
    "\n",
    "df2['logY1314'] =np.log(df2[\"Y\"])\n",
    "df2['logK1314'] =np.log(df2[\"K\"])\n",
    "df2['logL1314'] =np.log(df2[\"L\"])\n",
    "df2['logM1314'] =np.log(df2[\"M\"])\n",
    "\n",
    "diff=df2.groupby(df2.gvkey).diff()\n",
    "df2 =diff.dropna(how = \"all\")\n",
    "df2['constant']=1\n",
    "\n",
    "X1=df2[['constant','logK1314','logL1314','logM1314']]\n",
    "#Panel Data Regression\n",
    "ols3_1= sm.OLS(df2.logY1314,X1).fit(cov_type=\"HC0\")\n",
    "ols3_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant     0.026972\n",
      "deltalogK   -0.003537\n",
      "deltalogL    0.421159\n",
      "deltalogM    0.518474\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "           constant  deltalogK  deltalogL  deltalogM\n",
      "constant   0.000103  -0.000149  -0.000029   0.000043\n",
      "deltalogK -0.000149   0.002959  -0.001323  -0.001504\n",
      "deltalogL -0.000029  -0.001323   0.017899  -0.000772\n",
      "deltalogM  0.000043  -0.001504  -0.000772   0.005458\n",
      "\n",
      "\n",
      "                             Test for Constraints                             \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "c0             0.9361      0.138     -0.462      0.644       0.665       1.207\n",
      "==============================================================================\n",
      "we cannot reject the null hypothesis under 95% significance level.\n"
     ]
    }
   ],
   "source": [
    "#Coefficient vector and Covariance Matrix\n",
    "coef3= ols3.params\n",
    "cov3 = ols3.cov_params()\n",
    "\n",
    "print (coef3)\n",
    "print ('\\n')\n",
    "print (cov3)\n",
    "print ('\\n')\n",
    "\n",
    "#Hyptohesis Testing\n",
    "t = (sum(ols3.params[1:])-1)/np.sqrt(sum(np.diag(cov3))-cov3['constant'][0]+2*(cov3['deltalogL'][1]+cov3['deltalogM'][1]+cov3['deltalogM'][2]))\n",
    "t\n",
    "hypotheses = 'deltalogK+deltalogL+deltalogM=1'\n",
    "t_test = ols3.t_test(hypotheses)\n",
    "print(t_test)\n",
    "print(\"we cannot reject the null hypothesis under 95% significance level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3Q: How many firms are dropped due to a lack of information for 2013? Briefly comment on your results. Can you reject the null hypothesis of constant returns to scale? Do you believe your coefficient estimates are consistent for the underlying output elasticities? Why? How does this analysis differ from the cross-sectional one based on the 2014 data alone? What type of assumptions would justifiy this \"first differenced\" approach?\n",
    "\n",
    "A: In our dataset, we do not have observation in 2013 for some firms that we do in 2014, and vice versa, so we have dropped firms that are not present in both years, thus only getting left with 1270 observations. In fact, we have dropped 77 firms from year 2013 and 115 firms from 2014.\n",
    "\n",
    "From the regression table above we obtain values of the change in elasticity of output with respect to capital equal to -0.0035, the change in elasticity of output with respect to labor equal to 0.4212 and that with respect to materials expenditure 0.5185.\n",
    "\n",
    "The regression formula we are using in first difference approach is $\\Delta log Y_t=c_t+\\alpha \\Delta logK_t +\\beta \\Delta logL_t +\\gamma \\Delta logM_t + \\epsilon^*_t$, where $\\Delta log Y_t=log Y_t - logY_{t-1}$ (and similarly define $\\Delta logK_t$,$\\Delta logL_t$,$\\Delta logM_t$). $\\epsilon^*_t$ is the difference between $\\epsilon_{t}$ and $\\epsilon_{t-1}$ and interpreted as the growth rate of demeaned TFP from period t-1 to t.\n",
    "\n",
    "Since the t-value is approximately -0.46, we cannot reject the null hypothesis of constant returns to scale.\n",
    "\n",
    "\"First differenced\" approach would be reasonable if we take the Kaldor's theory of the firm, where $\\alpha$ and $\\beta$ are fixed over time (in his estimation 0.3 and 0.7). When we use first differenced approach with panel data we assume that all elasticities of output with respect to production factors are constant over time (or fixed effect). However, Picketty argues Kaldor's theory may not be true in reality, as we have observed an increasing income inequlality and possibly decreasing labor's share of income, which together suggest a possible increase in $\\alpha$ and decrease in $\\beta$ over time.\n",
    "\n",
    "If we follow Piketty's argument of possibly changing $\\alpha$ and $\\beta$, we might want to make a further assumption. For no endogeniety bias, we want to further assume that $\\Delta log K_t$ and $\\Delta\\epsilon^*_t$ are not correlated, and similarly that $\\Delta log L_t$ and $\\Delta\\epsilon^*_t$ are not corrleated. $\\Delta\\epsilon^*_t$ here could be interpreted as the growth rate of productivity, given the definition of $\\epsilon = log(A)-E[log(A)]$.\n",
    "This is a less strict assumption than the one for the cross sectional estimation based on 2014 data, since here we are assuming that the growth rate of capital does not covary with the growth rate of productivity and that the growth rate of labor does not covary with the growth rate of productivity. It might be that errors may be serially correlated as can be found in Olley-Pakes arguments.\n",
    "\n",
    "To sum up, \"first differenced\" approach would allow us to eliminate effect of unobserved variable that remain constant over time from our analysis since that might potentially bias our estimates. If we stick to a strong assumption of constant output elasticities of each factor and add the exogeniety assumption of zero covariance between dependent variables and the error term, then we would be able to obtain an unbiased estimate of our variables of interest. But it is doubtful whether the growth rates of log capital, log labor, and log materials expenditures are uncorrleated with the growth rate of the production. Our estimate might be potentially inconsistent for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>deltalogY</td>    <th>  R-squared:         </th>  <td>   0.219</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.166</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>-1.893e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 08 Mar 2018</td> <th>  Prob (F-statistic):</th>   <td>  1.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:14:24</td>     <th>  Log-Likelihood:    </th>  <td> -640.53</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1270</td>      <th>  AIC:               </th>  <td>   1445.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1188</td>      <th>  BIC:               </th>  <td>   1867.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    81</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC0</td>       <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th>  <td>    0.0294</td> <td>    0.019</td> <td>    1.568</td> <td> 0.117</td> <td>   -0.007</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deltalogK</th> <td>   -0.0172</td> <td>    0.056</td> <td>   -0.310</td> <td> 0.757</td> <td>   -0.126</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deltalogL</th> <td>    0.4143</td> <td>    0.137</td> <td>    3.019</td> <td> 0.003</td> <td>    0.145</td> <td>    0.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deltalogM</th> <td>    0.4794</td> <td>    0.073</td> <td>    6.602</td> <td> 0.000</td> <td>    0.337</td> <td>    0.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3111</th>   <td>   -0.0123</td> <td>    0.020</td> <td>   -0.618</td> <td> 0.537</td> <td>   -0.051</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3112</th>   <td>   -0.1074</td> <td>    0.024</td> <td>   -4.493</td> <td> 0.000</td> <td>   -0.154</td> <td>   -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3113</th>   <td>   -0.0819</td> <td>    0.030</td> <td>   -2.721</td> <td> 0.007</td> <td>   -0.141</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3114</th>   <td>   -0.0002</td> <td>    0.021</td> <td>   -0.010</td> <td> 0.992</td> <td>   -0.042</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3115</th>   <td>   -0.0182</td> <td>    0.088</td> <td>   -0.206</td> <td> 0.836</td> <td>   -0.191</td> <td>    0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3116</th>   <td>    0.0030</td> <td>    0.022</td> <td>    0.134</td> <td> 0.894</td> <td>   -0.040</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3118</th>   <td>   -0.0318</td> <td>    0.022</td> <td>   -1.425</td> <td> 0.154</td> <td>   -0.076</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3119</th>   <td>   -0.0861</td> <td>    0.055</td> <td>   -1.554</td> <td> 0.120</td> <td>   -0.195</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3121</th>   <td>   -0.0296</td> <td>    0.025</td> <td>   -1.162</td> <td> 0.245</td> <td>   -0.079</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3122</th>   <td>   -0.0114</td> <td>    0.056</td> <td>   -0.202</td> <td> 0.840</td> <td>   -0.122</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3131</th>   <td>   -0.0435</td> <td>    0.021</td> <td>   -2.028</td> <td> 0.043</td> <td>   -0.086</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3132</th>   <td>   -0.0303</td> <td>    0.023</td> <td>   -1.298</td> <td> 0.194</td> <td>   -0.076</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3141</th>   <td>    0.0700</td> <td>    0.096</td> <td>    0.728</td> <td> 0.467</td> <td>   -0.119</td> <td>    0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3151</th>   <td>    0.0253</td> <td>    0.085</td> <td>    0.297</td> <td> 0.767</td> <td>   -0.142</td> <td>    0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3152</th>   <td>   -0.0339</td> <td>    0.018</td> <td>   -1.900</td> <td> 0.057</td> <td>   -0.069</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3159</th>   <td>   -0.0579</td> <td>    0.021</td> <td>   -2.768</td> <td> 0.006</td> <td>   -0.099</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3162</th>   <td>   -0.0221</td> <td>    0.023</td> <td>   -0.962</td> <td> 0.336</td> <td>   -0.067</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3169</th>   <td>   -0.0386</td> <td>    0.023</td> <td>   -1.712</td> <td> 0.087</td> <td>   -0.083</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3211</th>   <td>   -0.4738</td> <td>    0.332</td> <td>   -1.427</td> <td> 0.154</td> <td>   -1.125</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3212</th>   <td>   -0.0342</td> <td>    0.046</td> <td>   -0.740</td> <td> 0.459</td> <td>   -0.125</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3219</th>   <td>   -0.0286</td> <td>    0.016</td> <td>   -1.742</td> <td> 0.082</td> <td>   -0.061</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3221</th>   <td>   -0.0006</td> <td>    0.026</td> <td>   -0.023</td> <td> 0.982</td> <td>   -0.051</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3222</th>   <td>   -0.0002</td> <td>    0.024</td> <td>   -0.009</td> <td> 0.993</td> <td>   -0.046</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3231</th>   <td>   -0.0170</td> <td>    0.021</td> <td>   -0.806</td> <td> 0.420</td> <td>   -0.058</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3241</th>   <td>    0.1514</td> <td>    0.125</td> <td>    1.207</td> <td> 0.228</td> <td>   -0.094</td> <td>    0.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3251</th>   <td>    0.0108</td> <td>    0.046</td> <td>    0.238</td> <td> 0.812</td> <td>   -0.079</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3252</th>   <td>   -0.0407</td> <td>    0.022</td> <td>   -1.849</td> <td> 0.064</td> <td>   -0.084</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3253</th>   <td>   -0.1308</td> <td>    0.051</td> <td>   -2.549</td> <td> 0.011</td> <td>   -0.231</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3254</th>   <td>    0.0958</td> <td>    0.058</td> <td>    1.639</td> <td> 0.101</td> <td>   -0.019</td> <td>    0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3255</th>   <td>   -0.0626</td> <td>    0.043</td> <td>   -1.470</td> <td> 0.142</td> <td>   -0.146</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3256</th>   <td>    0.0216</td> <td>    0.035</td> <td>    0.616</td> <td> 0.538</td> <td>   -0.047</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3259</th>   <td>   -0.0959</td> <td>    0.054</td> <td>   -1.783</td> <td> 0.075</td> <td>   -0.201</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3261</th>   <td>   -0.0270</td> <td>    0.019</td> <td>   -1.396</td> <td> 0.163</td> <td>   -0.065</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3262</th>   <td>   -0.0160</td> <td>    0.052</td> <td>   -0.305</td> <td> 0.761</td> <td>   -0.119</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3271</th>   <td>   -0.0032</td> <td>    0.034</td> <td>   -0.095</td> <td> 0.924</td> <td>   -0.070</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3272</th>   <td>   -0.0349</td> <td>    0.023</td> <td>   -1.517</td> <td> 0.129</td> <td>   -0.080</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3273</th>   <td>   -0.1191</td> <td>    0.064</td> <td>   -1.859</td> <td> 0.063</td> <td>   -0.245</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3274</th>   <td>    0.0045</td> <td>    0.028</td> <td>    0.159</td> <td> 0.873</td> <td>   -0.051</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3279</th>   <td>   -0.0945</td> <td>    0.043</td> <td>   -2.214</td> <td> 0.027</td> <td>   -0.178</td> <td>   -0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3311</th>   <td>   -0.0328</td> <td>    0.027</td> <td>   -1.207</td> <td> 0.227</td> <td>   -0.086</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3312</th>   <td>    0.0423</td> <td>    0.077</td> <td>    0.546</td> <td> 0.585</td> <td>   -0.110</td> <td>    0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3313</th>   <td>   -0.0178</td> <td>    0.040</td> <td>   -0.450</td> <td> 0.653</td> <td>   -0.095</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3314</th>   <td>   -0.0654</td> <td>    0.057</td> <td>   -1.156</td> <td> 0.248</td> <td>   -0.176</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3321</th>   <td>   -0.0446</td> <td>    0.018</td> <td>   -2.424</td> <td> 0.015</td> <td>   -0.081</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3322</th>   <td>   -0.0206</td> <td>    0.027</td> <td>   -0.772</td> <td> 0.440</td> <td>   -0.073</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3323</th>   <td>   -0.1513</td> <td>    0.106</td> <td>   -1.424</td> <td> 0.155</td> <td>   -0.360</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3324</th>   <td>    0.0471</td> <td>    0.093</td> <td>    0.504</td> <td> 0.614</td> <td>   -0.136</td> <td>    0.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3325</th>   <td>   -0.0276</td> <td>    0.029</td> <td>   -0.944</td> <td> 0.345</td> <td>   -0.085</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3326</th>   <td>   -0.0080</td> <td>    0.038</td> <td>   -0.214</td> <td> 0.831</td> <td>   -0.082</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3327</th>   <td>   -0.0291</td> <td>    0.019</td> <td>   -1.554</td> <td> 0.120</td> <td>   -0.066</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3328</th>   <td>   -0.0568</td> <td>    0.034</td> <td>   -1.649</td> <td> 0.099</td> <td>   -0.124</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3329</th>   <td>    0.0026</td> <td>    0.029</td> <td>    0.090</td> <td> 0.928</td> <td>   -0.054</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3331</th>   <td>   -0.0284</td> <td>    0.020</td> <td>   -1.423</td> <td> 0.155</td> <td>   -0.067</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3332</th>   <td>    0.0467</td> <td>    0.065</td> <td>    0.716</td> <td> 0.474</td> <td>   -0.081</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3333</th>   <td>   -0.0492</td> <td>    0.028</td> <td>   -1.760</td> <td> 0.078</td> <td>   -0.104</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3334</th>   <td>   -0.0151</td> <td>    0.018</td> <td>   -0.819</td> <td> 0.413</td> <td>   -0.051</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3335</th>   <td>   -0.0480</td> <td>    0.027</td> <td>   -1.776</td> <td> 0.076</td> <td>   -0.101</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3336</th>   <td>   -0.0232</td> <td>    0.025</td> <td>   -0.930</td> <td> 0.353</td> <td>   -0.072</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3339</th>   <td>   -0.0289</td> <td>    0.022</td> <td>   -1.294</td> <td> 0.196</td> <td>   -0.073</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3341</th>   <td>   -0.0643</td> <td>    0.024</td> <td>   -2.736</td> <td> 0.006</td> <td>   -0.110</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3342</th>   <td>   -0.0440</td> <td>    0.023</td> <td>   -1.907</td> <td> 0.056</td> <td>   -0.089</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3343</th>   <td>    0.0191</td> <td>    0.055</td> <td>    0.346</td> <td> 0.729</td> <td>   -0.089</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3344</th>   <td>    0.0049</td> <td>    0.023</td> <td>    0.212</td> <td> 0.832</td> <td>   -0.041</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3345</th>   <td>   -0.0162</td> <td>    0.022</td> <td>   -0.739</td> <td> 0.460</td> <td>   -0.059</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3346</th>   <td>   -0.1215</td> <td>    0.026</td> <td>   -4.705</td> <td> 0.000</td> <td>   -0.172</td> <td>   -0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3351</th>   <td>    0.0147</td> <td>    0.057</td> <td>    0.255</td> <td> 0.798</td> <td>   -0.098</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3352</th>   <td>   -0.0715</td> <td>    0.036</td> <td>   -1.994</td> <td> 0.046</td> <td>   -0.142</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3353</th>   <td>   -0.0308</td> <td>    0.035</td> <td>   -0.886</td> <td> 0.376</td> <td>   -0.099</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3359</th>   <td>   -0.0506</td> <td>    0.034</td> <td>   -1.505</td> <td> 0.132</td> <td>   -0.117</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3361</th>   <td>   -0.0132</td> <td>    0.022</td> <td>   -0.591</td> <td> 0.555</td> <td>   -0.057</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3362</th>   <td>   -0.0059</td> <td>    0.019</td> <td>   -0.302</td> <td> 0.763</td> <td>   -0.044</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3363</th>   <td>   -0.0046</td> <td>    0.019</td> <td>   -0.244</td> <td> 0.807</td> <td>   -0.041</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3364</th>   <td>   -0.0465</td> <td>    0.041</td> <td>   -1.123</td> <td> 0.261</td> <td>   -0.128</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3365</th>   <td>    0.0396</td> <td>    0.040</td> <td>    0.990</td> <td> 0.322</td> <td>   -0.039</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3366</th>   <td>   -0.0116</td> <td>    0.024</td> <td>   -0.479</td> <td> 0.632</td> <td>   -0.059</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3369</th>   <td>   -0.0377</td> <td>    0.033</td> <td>   -1.136</td> <td> 0.256</td> <td>   -0.103</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3371</th>   <td>    0.0147</td> <td>    0.034</td> <td>    0.426</td> <td> 0.670</td> <td>   -0.053</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n4_3372</th>   <td>   -0.0076</td> <td>    0.020</td> <td>   -0.371</td> <td> 0.710</td> <td>   -0.048</td> <td>    0.032</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1334.442</td> <th>  Durbin-Watson:     </th>  <td>   2.035</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>285990.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 4.556</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>75.949</td>  <th>  Cond. No.          </th>  <td>    191.</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              deltalogY   R-squared:                       0.219\n",
       "Model:                            OLS   Adj. R-squared:                  0.166\n",
       "Method:                 Least Squares   F-statistic:                -1.893e+14\n",
       "Date:                Thu, 08 Mar 2018   Prob (F-statistic):               1.00\n",
       "Time:                        17:14:24   Log-Likelihood:                -640.53\n",
       "No. Observations:                1270   AIC:                             1445.\n",
       "Df Residuals:                    1188   BIC:                             1867.\n",
       "Df Model:                          81                                         \n",
       "Covariance Type:                  HC0                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "constant       0.0294      0.019      1.568      0.117      -0.007       0.066\n",
       "deltalogK     -0.0172      0.056     -0.310      0.757      -0.126       0.092\n",
       "deltalogL      0.4143      0.137      3.019      0.003       0.145       0.683\n",
       "deltalogM      0.4794      0.073      6.602      0.000       0.337       0.622\n",
       "n4_3111       -0.0123      0.020     -0.618      0.537      -0.051       0.027\n",
       "n4_3112       -0.1074      0.024     -4.493      0.000      -0.154      -0.061\n",
       "n4_3113       -0.0819      0.030     -2.721      0.007      -0.141      -0.023\n",
       "n4_3114       -0.0002      0.021     -0.010      0.992      -0.042       0.042\n",
       "n4_3115       -0.0182      0.088     -0.206      0.836      -0.191       0.154\n",
       "n4_3116        0.0030      0.022      0.134      0.894      -0.040       0.046\n",
       "n4_3118       -0.0318      0.022     -1.425      0.154      -0.076       0.012\n",
       "n4_3119       -0.0861      0.055     -1.554      0.120      -0.195       0.022\n",
       "n4_3121       -0.0296      0.025     -1.162      0.245      -0.079       0.020\n",
       "n4_3122       -0.0114      0.056     -0.202      0.840      -0.122       0.099\n",
       "n4_3131       -0.0435      0.021     -2.028      0.043      -0.086      -0.001\n",
       "n4_3132       -0.0303      0.023     -1.298      0.194      -0.076       0.015\n",
       "n4_3141        0.0700      0.096      0.728      0.467      -0.119       0.259\n",
       "n4_3151        0.0253      0.085      0.297      0.767      -0.142       0.193\n",
       "n4_3152       -0.0339      0.018     -1.900      0.057      -0.069       0.001\n",
       "n4_3159       -0.0579      0.021     -2.768      0.006      -0.099      -0.017\n",
       "n4_3162       -0.0221      0.023     -0.962      0.336      -0.067       0.023\n",
       "n4_3169       -0.0386      0.023     -1.712      0.087      -0.083       0.006\n",
       "n4_3211       -0.4738      0.332     -1.427      0.154      -1.125       0.177\n",
       "n4_3212       -0.0342      0.046     -0.740      0.459      -0.125       0.056\n",
       "n4_3219       -0.0286      0.016     -1.742      0.082      -0.061       0.004\n",
       "n4_3221       -0.0006      0.026     -0.023      0.982      -0.051       0.050\n",
       "n4_3222       -0.0002      0.024     -0.009      0.993      -0.046       0.046\n",
       "n4_3231       -0.0170      0.021     -0.806      0.420      -0.058       0.024\n",
       "n4_3241        0.1514      0.125      1.207      0.228      -0.094       0.397\n",
       "n4_3251        0.0108      0.046      0.238      0.812      -0.079       0.100\n",
       "n4_3252       -0.0407      0.022     -1.849      0.064      -0.084       0.002\n",
       "n4_3253       -0.1308      0.051     -2.549      0.011      -0.231      -0.030\n",
       "n4_3254        0.0958      0.058      1.639      0.101      -0.019       0.210\n",
       "n4_3255       -0.0626      0.043     -1.470      0.142      -0.146       0.021\n",
       "n4_3256        0.0216      0.035      0.616      0.538      -0.047       0.090\n",
       "n4_3259       -0.0959      0.054     -1.783      0.075      -0.201       0.010\n",
       "n4_3261       -0.0270      0.019     -1.396      0.163      -0.065       0.011\n",
       "n4_3262       -0.0160      0.052     -0.305      0.761      -0.119       0.087\n",
       "n4_3271       -0.0032      0.034     -0.095      0.924      -0.070       0.063\n",
       "n4_3272       -0.0349      0.023     -1.517      0.129      -0.080       0.010\n",
       "n4_3273       -0.1191      0.064     -1.859      0.063      -0.245       0.006\n",
       "n4_3274        0.0045      0.028      0.159      0.873      -0.051       0.060\n",
       "n4_3279       -0.0945      0.043     -2.214      0.027      -0.178      -0.011\n",
       "n4_3311       -0.0328      0.027     -1.207      0.227      -0.086       0.020\n",
       "n4_3312        0.0423      0.077      0.546      0.585      -0.110       0.194\n",
       "n4_3313       -0.0178      0.040     -0.450      0.653      -0.095       0.060\n",
       "n4_3314       -0.0654      0.057     -1.156      0.248      -0.176       0.046\n",
       "n4_3321       -0.0446      0.018     -2.424      0.015      -0.081      -0.009\n",
       "n4_3322       -0.0206      0.027     -0.772      0.440      -0.073       0.032\n",
       "n4_3323       -0.1513      0.106     -1.424      0.155      -0.360       0.057\n",
       "n4_3324        0.0471      0.093      0.504      0.614      -0.136       0.230\n",
       "n4_3325       -0.0276      0.029     -0.944      0.345      -0.085       0.030\n",
       "n4_3326       -0.0080      0.038     -0.214      0.831      -0.082       0.066\n",
       "n4_3327       -0.0291      0.019     -1.554      0.120      -0.066       0.008\n",
       "n4_3328       -0.0568      0.034     -1.649      0.099      -0.124       0.011\n",
       "n4_3329        0.0026      0.029      0.090      0.928      -0.054       0.059\n",
       "n4_3331       -0.0284      0.020     -1.423      0.155      -0.067       0.011\n",
       "n4_3332        0.0467      0.065      0.716      0.474      -0.081       0.175\n",
       "n4_3333       -0.0492      0.028     -1.760      0.078      -0.104       0.006\n",
       "n4_3334       -0.0151      0.018     -0.819      0.413      -0.051       0.021\n",
       "n4_3335       -0.0480      0.027     -1.776      0.076      -0.101       0.005\n",
       "n4_3336       -0.0232      0.025     -0.930      0.353      -0.072       0.026\n",
       "n4_3339       -0.0289      0.022     -1.294      0.196      -0.073       0.015\n",
       "n4_3341       -0.0643      0.024     -2.736      0.006      -0.110      -0.018\n",
       "n4_3342       -0.0440      0.023     -1.907      0.056      -0.089       0.001\n",
       "n4_3343        0.0191      0.055      0.346      0.729      -0.089       0.127\n",
       "n4_3344        0.0049      0.023      0.212      0.832      -0.041       0.051\n",
       "n4_3345       -0.0162      0.022     -0.739      0.460      -0.059       0.027\n",
       "n4_3346       -0.1215      0.026     -4.705      0.000      -0.172      -0.071\n",
       "n4_3351        0.0147      0.057      0.255      0.798      -0.098       0.127\n",
       "n4_3352       -0.0715      0.036     -1.994      0.046      -0.142      -0.001\n",
       "n4_3353       -0.0308      0.035     -0.886      0.376      -0.099       0.037\n",
       "n4_3359       -0.0506      0.034     -1.505      0.132      -0.117       0.015\n",
       "n4_3361       -0.0132      0.022     -0.591      0.555      -0.057       0.031\n",
       "n4_3362       -0.0059      0.019     -0.302      0.763      -0.044       0.032\n",
       "n4_3363       -0.0046      0.019     -0.244      0.807      -0.041       0.032\n",
       "n4_3364       -0.0465      0.041     -1.123      0.261      -0.128       0.035\n",
       "n4_3365        0.0396      0.040      0.990      0.322      -0.039       0.118\n",
       "n4_3366       -0.0116      0.024     -0.479      0.632      -0.059       0.036\n",
       "n4_3369       -0.0377      0.033     -1.136      0.256      -0.103       0.027\n",
       "n4_3371        0.0147      0.034      0.426      0.670      -0.053       0.082\n",
       "n4_3372       -0.0076      0.020     -0.371      0.710      -0.048       0.032\n",
       "==============================================================================\n",
       "Omnibus:                     1334.442   Durbin-Watson:                   2.035\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           285990.496\n",
       "Skew:                           4.556   Prob(JB):                         0.00\n",
       "Kurtosis:                      75.949   Cond. No.                         191.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
       "\"\"\""
      ]
     },
     "execution_count": 922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.3 sector specific dummy variables added\n",
    "\n",
    "# Form dummies for included matched SAT/CEB pairs\n",
    "pair_dumms = pd.get_dummies(df_13['naics_4digit'].astype(\"category\"), prefix=\"n4\")\n",
    "\n",
    "df_13['constant'] =1\n",
    "df_13['deltalogY'] = df_14.logY14-df_13.logY13\n",
    "df_13['deltalogK'] = df_14.logK14-df_13.logK13\n",
    "df_13['deltalogL'] = df_14.logL14-df_13.logL13\n",
    "df_13['deltalogM'] = df_14.logM14-df_13.logM13\n",
    "X1=df_13[['constant','deltalogK','deltalogL','deltalogM']]\n",
    "\n",
    "X1 = pd.concat([X1, pair_dumms.iloc[:,0:-1]], axis=1)\n",
    "#Panel Data Regression with dummy variables\n",
    "ols4=sm.OLS(df_13.deltalogY,X1).fit(cov_type=\"HC0\")\n",
    "ols4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Test for Constraints                             \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "c0             0.8764      0.138     -0.897      0.369       0.606       1.146\n",
      "==============================================================================\n",
      "we cannot reject the null hypothesis under 95% significance level.\n"
     ]
    }
   ],
   "source": [
    "#Coefficient vector and covariance matrix\n",
    "coef4= ols4.params\n",
    "cov4 = ols4.cov_params()\n",
    "\n",
    "#Hyptohesis Testing\n",
    "t = (sum(ols4.params[1:4])-1)/np.sqrt(cov4['deltalogK'][1]+cov4['deltalogL'][2]+cov4['deltalogM'][3]+2*(cov4['deltalogL'][1]+cov4['deltalogM'][1]+cov4['deltalogM'][2]))\n",
    "t\n",
    "hypotheses = 'deltalogK+deltalogL+deltalogM=1'\n",
    "t_test = ols4.t_test(hypotheses)\n",
    "print(t_test)\n",
    "print(\"we cannot reject the null hypothesis under 95% significance level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3\n",
    "Q: Comment on these results.\n",
    "\n",
    "A: Here we are doing \"first differenced\" approach again, this time with dummy variables included, though.\n",
    "All the assumptions for OLS exogeniety and the assumption of constant output elasticities of each factor (or constant share of labor, capital, and materials expenditure in the output) we used for the \"first differenced\" approach before remain the same,\n",
    "and for the effect of adding dummy variables to the regression, we are fixing industry-specific effect that might bias the estimates.\n",
    "From the regression table above we obtain $\\alpha = -0.017$, $\\beta = 0.4142$ and $\\gamma = 0.4793$.\n",
    "Here, $\\alpha = -0.017$ would mean that -0.0172% changes in growth rate of Y is induced by 1% change in growth rate of K, holding other variables constant.\n",
    "Since the test statistic value is approximately -0.89, we cannot reject the null hypothesis of constant returns to scale.\n",
    "Also, since the growth rate of capital, labor, and materials expenditure could well be correlated with the growth rate of productivity, there still exists possibility that we might not get consistent estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  import sys\n",
      "/Users/davidson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>logVA13</td>     <th>  R-squared:         </th> <td>   0.926</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.926</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2135.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 08 Mar 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:14:25</td>     <th>  Log-Likelihood:    </th> <td> -961.94</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1046</td>      <th>  AIC:               </th> <td>   1938.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1039</td>      <th>  BIC:               </th> <td>   1973.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC0</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logL13</th>    <td>    0.8599</td> <td>    0.028</td> <td>   31.183</td> <td> 0.000</td> <td>    0.806</td> <td>    0.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th>  <td>    3.4470</td> <td>    0.153</td> <td>   22.536</td> <td> 0.000</td> <td>    3.147</td> <td>    3.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logK13</th>    <td>    0.2315</td> <td>    0.045</td> <td>    5.108</td> <td> 0.000</td> <td>    0.143</td> <td>    0.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>i13</th>       <td>    0.0005</td> <td>    0.000</td> <td>    1.634</td> <td> 0.102</td> <td>   -0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logK13_sq</th> <td>   -0.0036</td> <td>    0.003</td> <td>   -1.067</td> <td> 0.286</td> <td>   -0.010</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>i13_sq</th>    <td> 1.033e-09</td> <td> 1.91e-09</td> <td>    0.542</td> <td> 0.588</td> <td> -2.7e-09</td> <td> 4.77e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logK13i</th>   <td>-4.056e-05</td> <td> 2.81e-05</td> <td>   -1.441</td> <td> 0.150</td> <td>-9.57e-05</td> <td> 1.46e-05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>703.103</td> <th>  Durbin-Watson:     </th> <td>   1.972</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>32163.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.480</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>29.709</td>  <th>  Cond. No.          </th> <td>3.09e+08</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                logVA13   R-squared:                       0.926\n",
       "Model:                            OLS   Adj. R-squared:                  0.926\n",
       "Method:                 Least Squares   F-statistic:                     2135.\n",
       "Date:                Thu, 08 Mar 2018   Prob (F-statistic):               0.00\n",
       "Time:                        17:14:25   Log-Likelihood:                -961.94\n",
       "No. Observations:                1046   AIC:                             1938.\n",
       "Df Residuals:                    1039   BIC:                             1973.\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:                  HC0                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "logL13         0.8599      0.028     31.183      0.000       0.806       0.914\n",
       "constant       3.4470      0.153     22.536      0.000       3.147       3.747\n",
       "logK13         0.2315      0.045      5.108      0.000       0.143       0.320\n",
       "i13            0.0005      0.000      1.634      0.102      -0.000       0.001\n",
       "logK13_sq     -0.0036      0.003     -1.067      0.286      -0.010       0.003\n",
       "i13_sq      1.033e-09   1.91e-09      0.542      0.588    -2.7e-09    4.77e-09\n",
       "logK13i    -4.056e-05   2.81e-05     -1.441      0.150   -9.57e-05    1.46e-05\n",
       "==============================================================================\n",
       "Omnibus:                      703.103   Durbin-Watson:                   1.972\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            32163.325\n",
       "Skew:                          -2.480   Prob(JB):                         0.00\n",
       "Kurtosis:                      29.709   Cond. No.                     3.09e+08\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
       "[2] The condition number is large, 3.09e+08. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.4\n",
    "#Create log variable for each inputs for 2013\n",
    "df_13['logY13'] = np.log(df_13['Y'])\n",
    "df_13['logK13'] = np.log(df_13['K'])\n",
    "df_13['logL13'] =np.log(df_13['L'])\n",
    "df_13['logM13'] =np.log(df_13['M'])\n",
    "df_13['logVA13'] =np.log(df_13['VA'])\n",
    "#To Construct W for 2013\n",
    "df_13['i13'] = df_13['i']\n",
    "df_13['logK13_sq'] = np.square(df_13[\"logK13\"])\n",
    "df_13['i13_sq'] = np.square(df_13[\"i\"])\n",
    "df_13['logK13i'] = df_13[\"logK13\"]*df_13[\"i\"]\n",
    "\n",
    "#Create log variable for each inputs for 2014\n",
    "df_14['logY14'] = np.log(df_14['Y'])\n",
    "df_14['logK14'] = np.log(df_14['K'])\n",
    "df_14['logL14'] = np.log(df_14['L'])\n",
    "df_14['logM14'] = np.log(df_14['M'])\n",
    "df_14['logVA14'] =np.log(df_14['VA'])\n",
    "df_14['VA14'] = df_14['VA']\n",
    "df_14['K14'] = df_14['K']\n",
    "df_14['L14'] = df_14['L']\n",
    "#To Construct W for 2014\n",
    "df_14['i14'] = df_14['i']\n",
    "df_14['logK14_sq'] = np.square(df_14[\"logK14\"])\n",
    "df_14['i14_sq'] = np.square(df_14[\"i\"])\n",
    "df_14['logK14i'] = df_14[\"logK14\"]*df_14[\"i\"]\n",
    "\n",
    "df_1314 = pd.concat([df_13,df_14], axis=1)\n",
    "df_1314 = df_1314.dropna()\n",
    "df_1314['constant'] = 1\n",
    "#Construct W for 2013 and W for 2014 \n",
    "W13 = df_1314[['constant', 'logK13', 'i13', 'logK13_sq', 'i13_sq', 'logK13i']]\n",
    "W14 = df_1314[['constant', 'logK14', 'i14', 'logK14_sq', 'i14_sq', 'logK14i']]\n",
    "#Concatenate the W for the two years\n",
    "X2 = pd.concat([df_1314['logL13'], W13], axis=1)\n",
    "#Computing the sample analog of equation using the 2013 data(1)\n",
    "ols5=sm.OLS(df_1314.logVA13,X2).fit(cov_type=\"HC0\")\n",
    "ols5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4Q: Under what conditions does ${b}_0$ coincide with the elasticity of ouput with  respect to labor, ${\\beta}_0$?\n",
    "Please Provide a precise set of conditions as well as an informal discussion of the underlying economics.\n",
    "\n",
    "A: For the estimate ${b}_0$ to coincide with ${\\beta}_0$ in Proxy Variable Regression, we need the conditonal uncorrelatedness assumption. Namely, we assume $E^*[logA|logL,W] = E^*[logA|W]$. We additionally assume that $E[{u}_t|log{A}_0, log{A}_1,..., log{A}_{t-1}] = 0$\n",
    "and that $K_t$ = (1-$\\delta$)*$K_{t-1}$ = ${i}_{t-1}$. We can make assumptions using economic decay when we follow \"structural approach\" as in Olley and Pakes (1996). The first additional assumption comes after representing $log{A}_t$ = $\\lambda$+$\\rho log{A}_{t-1}$+${u}_t$. The second additional assumption, also known as, \"Law of Output\" basically states that investment decision is made in period t-1. Regarding this we also assume that any productivity shock in period t, or ${V}_t$, is not correlated with the information available in period t-1, or ${I}_{t-1}$, since the former was unknown when the latter was given. This can lead to a good condition of $C({logK}_t,{V}_t)$=0 for preventing endogeniety bias. However, if $C({logL}_t,{V}_t)$ is not zero, we still have a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4Q: Describe this TFP process in words. Next show that $log{VA}_t - {\\beta}_0log{L}_{t} = {\\lambda}_0+{\\alpha}_0log{K}_t+{\\rho}_0({\\phi}_{t-1}-{\\alpha}_0ln{K}_{t-1})+{V}_t$.\n",
    "\n",
    "A: To explain the TFP process, the current productivity ${A}_t$ evolves with the influence of or depends on the past productivity ${A}_{t-1}$ and the unforecastable innovation to firm log TFP, as shown in $E[ln{A}_t|{I}_{t-1}]$ = ${\\lambda}_0$-$\\rho lnA_{t-1}$ and ${V}_t$ = $ln{A}_t$- ${\\lambda}_0$-$\\rho lnA_{t-1}$\n",
    "\n",
    "A: \n",
    "Next, we want to show $log{VA}_t - {\\beta}_0log{L}_{t} = {\\lambda}_0+{\\alpha}_0log{K}_t+{\\rho}_0({\\phi}_{t-1}-{\\alpha}_0ln{K}_{t-1})+{V}_t$.\n",
    "Since $ln {VA}_t = {\\alpha}_0 ln{K}_t + {\\beta}_0 ln{L}_t + ln{A}_t$, and ${V}_t = ln{A}_t-{\\lambda}_0-{\\rho}ln{A}_{t-1}$, we only need to show $log{A}_{t-1}$ = ${\\phi}_{t-1}-\\alpha log{K}_{t-1}$.\n",
    "\n",
    "Since $E^*[{logA}_t|{logL}_t, {W}_t] =E^*[{logA}_t|{W}_t] = \\sum_{j=1}^6 {\\Pi}_{tj}{W}_{jt} = {\\phi}_t-{α}_0logKt$, which is approximately equal to $log {A}_t$, we can plug in $log {A}_{t-1}$= ${\\phi}_{t-1}-{\\alpha}_0 log{K}_{t-1}$ derived from the first stage form into the second stage form of PVR, which is given as $log{VA}_t = {\\lambda}_0+{\\alpha}_0 log{K}_t + {\\beta}_0 log{L}_t + {\\rho}_0 log{A}_{t-1} + {V}_t$. Here, since capital is already included into our vector of proxy variables W, our linear predictor will become:\n",
    "$E^∗[ln{VA}_t|ln{L}_t,W] = βlog{L}_t+\\sum_{j=1}^6 {\\Pi}_{tj}{W}_{jt} = βlog{L}_t+{\\phi}_{t}$\n",
    " \n",
    "(Here I follow ${\\phi}_{t}={α}_0logKt+\\sum_{j=1}^6 {\\Pi}_{tj}{W}_{jt}$, from the first step of the regression.)\n",
    "\n",
    "Now we get the wanted regression sentence and can compute OLS fit of $log {VA}_t$-$\\hat{\\beta}_0log{L}_{t}$ onto constant, $log{K}_t$, $log{K}_{t-1}$, and $\\hat{\\phi}_{t-1}$, the last one of which we obtain from computation step 2. We also use the same $\\hat{\\beta}_0$ that we obtained from the first step of computing OLS fit of $log {VA}_{t-1}$ onto $log {L}_{t-1}$ and ${W}_{t-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85986810811088321"
      ]
     },
     "execution_count": 925,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.4.1 Using the 2013 data compute the sample analog of equation (1).\n",
    "#Estimate of b_0\n",
    "ols5.params[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Discuss your estimate of ${b}_0$\n",
    "\n",
    "A: This estimate of ${b}_0$ might be biased if we reasonably assume that labor, unlike capital in firm input decision, is freely adjustable at the time as the productivity information gets available to firms. In other words, if $C({logL}_t,{V}_t)$ is not zero, that might bias the estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       4.442538\n",
       "1       4.869388\n",
       "2       5.240929\n",
       "3       4.309519\n",
       "4       4.257229\n",
       "5       4.785508\n",
       "6       4.055761\n",
       "7       5.404554\n",
       "8       5.385090\n",
       "9       5.302124\n",
       "10      5.164404\n",
       "11      4.926091\n",
       "12      5.031770\n",
       "13      5.232564\n",
       "14      6.327484\n",
       "15      4.370604\n",
       "16      5.410098\n",
       "17      4.729967\n",
       "19      4.351595\n",
       "20      7.228743\n",
       "21      5.325618\n",
       "22      4.790462\n",
       "23      4.625856\n",
       "24      5.379474\n",
       "25      4.017128\n",
       "26      5.347931\n",
       "27      4.237491\n",
       "28      4.621511\n",
       "29      5.483726\n",
       "30      4.828054\n",
       "          ...   \n",
       "1223    4.828758\n",
       "1224    4.405065\n",
       "1225    4.953143\n",
       "1226    5.187213\n",
       "1229    4.341808\n",
       "1230    1.331857\n",
       "1233    4.931027\n",
       "1235    5.384281\n",
       "1236    4.747448\n",
       "1237    3.030578\n",
       "1238    6.146896\n",
       "1242    3.554810\n",
       "1243    5.822969\n",
       "1244    4.834081\n",
       "1245    4.030231\n",
       "1246    3.584109\n",
       "1247    5.014510\n",
       "1248    5.486155\n",
       "1249    4.803897\n",
       "1250    2.325542\n",
       "1251    5.029350\n",
       "1252    4.972482\n",
       "1254    5.356135\n",
       "1256    4.383462\n",
       "1259    4.683086\n",
       "1263    4.430977\n",
       "1264    3.516663\n",
       "1266    4.716761\n",
       "1267    3.459649\n",
       "1269    5.304223\n",
       "Length: 1046, dtype: float64"
      ]
     },
     "execution_count": 926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.4.2 Construct, for each firm, the estimate of phihat2013.\n",
    "b_0=ols5.params[0]\n",
    "phihat13 = df_1314['logVA13']-b_0*df_1314['logL13']\n",
    "phihat13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.707</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.706</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   339.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 08 Mar 2018</td> <th>  Prob (F-statistic):</th> <td>9.69e-154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:14:25</td>     <th>  Log-Likelihood:    </th> <td> -509.64</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1046</td>      <th>  AIC:               </th> <td>   1027.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1042</td>      <th>  BIC:               </th> <td>   1047.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC0</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>constant</th> <td>    1.3419</td> <td>    0.337</td> <td>    3.987</td> <td> 0.000</td> <td>    0.682</td> <td>    2.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logK14</th>   <td>    0.0125</td> <td>    0.080</td> <td>    0.157</td> <td> 0.875</td> <td>   -0.144</td> <td>    0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>logK13</th>   <td>    0.0561</td> <td>    0.082</td> <td>    0.688</td> <td> 0.492</td> <td>   -0.104</td> <td>    0.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>phihat13</th> <td>    0.6372</td> <td>    0.090</td> <td>    7.043</td> <td> 0.000</td> <td>    0.460</td> <td>    0.815</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>425.723</td> <th>  Durbin-Watson:     </th> <td>   1.833</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>35862.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.935</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>31.624</td>  <th>  Cond. No.          </th> <td>    64.4</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.707\n",
       "Model:                            OLS   Adj. R-squared:                  0.706\n",
       "Method:                 Least Squares   F-statistic:                     339.5\n",
       "Date:                Thu, 08 Mar 2018   Prob (F-statistic):          9.69e-154\n",
       "Time:                        17:14:25   Log-Likelihood:                -509.64\n",
       "No. Observations:                1046   AIC:                             1027.\n",
       "Df Residuals:                    1042   BIC:                             1047.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:                  HC0                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "constant       1.3419      0.337      3.987      0.000       0.682       2.001\n",
       "logK14         0.0125      0.080      0.157      0.875      -0.144       0.169\n",
       "logK13         0.0561      0.082      0.688      0.492      -0.104       0.216\n",
       "phihat13       0.6372      0.090      7.043      0.000       0.460       0.815\n",
       "==============================================================================\n",
       "Omnibus:                      425.723   Durbin-Watson:                   1.833\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            35862.177\n",
       "Skew:                           0.935   Prob(JB):                         0.00\n",
       "Kurtosis:                      31.624   Cond. No.                         64.4\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
       "\"\"\""
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.4.3 Compute the OLS fit (Proxy Variable Regrssion)\n",
    "\n",
    "df_1314['phihat13'] = phihat13\n",
    "\n",
    "Y1 = df_1314['logVA14']-b_0*df_1314['logL14']\n",
    "X2 = pd.concat([df_1314[['constant','logK14']], df_1314[['logK13', 'phihat13']]], axis=1)\n",
    "\n",
    "ols6=sm.OLS(Y1,X2).fit(cov_type=\"HC0\")\n",
    "ols6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are your implied estimates of alpha_0, beta_0, and rho_0\n",
      "0.0124778553266\n",
      "0.859868108111\n",
      "0.63721627841\n"
     ]
    }
   ],
   "source": [
    "#1.4 estimates of alpha_0, beta_0, and rho_0\n",
    "alpha_hat = ols6.params[1]\n",
    "beta_hat = ols5.params[0]\n",
    "rho_hat = ols6.params[3]\n",
    "\n",
    "print(\"What are your implied estimates of alpha_0, beta_0, and rho_0\")\n",
    "print(alpha_hat)\n",
    "print(beta_hat)\n",
    "print(rho_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4Q: What are your implied estimates of ${\\alpha}_0, {\\beta}_0 and {\\rho}_0?$ Discuss your estimation procedure and the assumptions justifying it.\n",
    "\n",
    "A: my implied estimates of ${\\alpha}_0$, ${\\beta}_0$, and ${\\rho}_0$ are 0.0125, 0.8599, and\n",
    "0.6372, respectively. \n",
    "\n",
    "We start with year 2013 data to do the first step of the proxy variable regression.\n",
    "\n",
    "$E^*[log{VA}_{t-1}|log{L}_{t-1},{W}_{t-1}]$ \n",
    "$$=E^*[{\\alpha}_0log{K}_{t-1}+{\\beta}_0log{L}_{t-1}|log{L}_{t-1},{W}_{t-1}]$$\n",
    "$$={\\beta}_0log{L}_{t-1}+{\\alpha}_0log{K}_{t-1}+E^*[log{A}_{t-1}|log{L}_{t-1},{W}_{t-1}]$$\n",
    "$$={\\beta}_0log{L}_{t-1}+{\\alpha}_0log{K}_{t-1}+E^*[log{A}_{t-1}|{W}_{t-1}]$$                               \n",
    "$$={\\beta}_0log{L}_{t-1}+({\\alpha}_0log{K}_{t-1}+\\sum_{j=1}^6 {\\Pi}_{tj}{W}_{jt})$$\n",
    "$$={\\beta}_0log{L}_{t-1}+{\\phi}_{t-1}$$\n",
    "\n",
    "Now we can compute OLS fit of $log{VA}_{t-1}$ onto $log{L}_{t-1}$ and $W_{t-1}$, and get an estimates $\\hat\\beta_0$ and $\\hat\\phi_{t-1}$.\n",
    "\n",
    "My estimation procedure comes from using the second step of the proxy variable regression. The second step is given as $log{VA}_t = {\\lambda}_0+{\\alpha}_0 log{K}_t + {\\beta}_0 log{L}_t + {\\rho}_0 log{A}_{t-1} + {V}_t$. Followig the explanation above, we can plug in $log {A}_{t-1}$= ${\\phi}_{t-1}-\\alpha log{K}_{t-1}$. Then we get $log{VA}_t - {\\beta}_0log{L}_{t} = {\\lambda}_0+{\\alpha}_0log{K}_t+{\\rho}_0{\\phi}_{t-1}-{\\rho}_0{\\alpha}_0ln{K}_{t-1}+{V}_t$,\n",
    "so we know ${\\alpha}_0$ is the coefficient on $log{K}_t$, ${\\beta}_0$ the coefficient on $log{L}_{t}$ from the first\n",
    "step of the regression(the coefficient on $log{L}_{t-1}$), and the ${\\rho}_0$ is the coefficient on ${\\phi}_{t-1}$.\n",
    "\n",
    "One assumption is the 2nd assumption of the proxy variable regression, namely conditional uncorrelatedness. We can write this as $E^*[logA|logL,W]$ = $E^*[logA|W]$. Also, we assume that ${\\beta}_0$ does not change over time, and we use ${\\beta}_0$ to construct a dependent variable in the second step.\n",
    "One concern here is that covariance of ${L}_t$ and ${v}_t$ might not be zero because labor, unlike capital, is reasonably assumed to be relatively freely adjustable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4Q: You need to select one of your production function estimates as your preferred one. State which one you are using and why. Write a few paragraphs summarizing analysis so far.\n",
    "\n",
    "Ans: I am using proxy variable production function estimates since the proxy variable regression might capture reality better under assumptions and might hopefully lead to a less biased estimate. OLS and \"first differenced\" approaches seem to rely on stronger general econometric assumptions than the proxy variable approach. Productivity is assumed to be uncorrelated with capital and labor, either in level terms or growth rate terms. On the other hand, the proxy variable approach adopts stronger assumptions on the economic nature of the firm's production functions. Here, productivity is given as a function of capital and investments that are slowly adjustable, while labor is rather more promptly adjusted. I believe this reflects the true nature of firms and market conditions.\n",
    "\n",
    "We review the analysis we did for the proxy variable regression here.\n",
    "\n",
    "According to Olley and Pakes (1996), in order to recover an unbiased estimates for firm's production function, we introduce three further assumptions on the economic nature of the firm: \n",
    "\n",
    "First, the evolution process of productivity depends only on its previous productivity level and shock, as described in the function:  $ln{A}_t={δ}_0+ρln{A}_{t−1}+{u}_t$, where  $E[{u}_t|ln{A}_0,...,ln{A}_{t−1}]=0$.\n",
    "\n",
    "Second, the firm can adjust the level of capital slowly. The level of investments i is chosen before observing the level of productivity, and the law of output gives ${K}_t=(1−δ)K_{t−1}+i_{t−1}$. Then, firms decide on investment level given the estimation of productivity in the next period, as shown in $E[{A}_t|{I}_{t−1}]$, where I is the set of available informations in  t−1. With this assumption the difference between actual productivity and estimated is set equal to Vt, and the covariance between the level of investments in t−1 and  Vt, as the covariance between  Kt and  Vt, is equal to zero. \n",
    "\n",
    "Third, labor can be freely adjusted even after observing the shift in productivity.\n",
    "With these structural assumptions on capital, labor, and the evolution of productivity we can further develop our analysis. In the first stage we have:\n",
    "\n",
    "$E^*[log{VA}_{t-1}|log{L}_{t-1},{W}_{t-1}]$ \n",
    "$$=E^*[{\\alpha}_0log{K}_{t-1}+{\\beta}_0log{L}_{t-1}|log{L}_{t-1},{W}_{t-1}]$$\n",
    "$$={\\beta}_0log{L}_{t-1}+{\\alpha}_0log{K}_{t-1}+E^*[log{A}_{t-1}|log{L}_{t-1},{W}_{t-1}]$$\n",
    "$$={\\beta}_0log{L}_{t-1}+{\\alpha}_0log{K}_{t-1}+E^*[log{A}_{t-1}|{W}_{t-1}]$$                               \n",
    "$$={\\beta}_0log{L}_{t-1}+({\\alpha}_0log{K}_{t-1}+\\sum_{j=1}^6 {\\Pi}_{tj}{W}_{jt})$$\n",
    "$$={\\beta}_0log{L}_{t-1}+{\\phi}_{t-1}$$\n",
    " \n",
    "Here, since capital is already included into our vector of proxy variables W, our linear predictor will become:\n",
    "$E^∗[ln{VA}_t|ln{L}_t,W] = βlog{L}_t+\\sum_{j=1}^6 {\\Pi}_{tj}{W}_{jt} = βlog{L}_t+{\\phi}_{t}$\n",
    "\n",
    "Then, under our initial assumptions, {b}_0 will coincide with {β}_0. This way we recover an estimate for the elasticity of output with respect to labor.\n",
    "In order to recover the elasticity of output with respect to capital, we use first stage estimated coefficients $\\hat\\beta_0$ and $\\hat\\phi_{t-1}$ along with the first assumption on productivity in the second stage regression:\n",
    "\n",
    "$ln{VA}_t=αlog{K}_t+βlog{L}_t+log{A}_t$\n",
    " \n",
    "$ln{VA}_t−βlog{L}_t={\\lambda}_0+αlog{K}_t+ρ{lnA}_{t−1}+{V}_t$\n",
    " \n",
    "$ln{VA}_t−βlog{L}_t={\\lambda}_0+αlog{K}_t+{ρ}({\\phi}_{t−1}({K}_{t−1},{i}_{t−1})−{α}_0log{K}_{t−1})+{V}_t$\n",
    " \n",
    "This way we can recover an unbiased estimates for the elasticity of output with respect to capital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZ9BLUucMeknqnEEvSZ0z6CWpc/8PxUfV8oeMxzcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1b849320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "count    1270.000000\n",
      "mean        8.496204\n",
      "std         4.226131\n",
      "min         0.024059\n",
      "25%         6.216631\n",
      "50%         7.815732\n",
      "75%         9.988043\n",
      "max        39.505580\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#1.5 Total Factor Productivity estimates--CD version just for comparison\n",
    "\n",
    "Y=df_14['Y']\n",
    "\n",
    "K=df_14['K']\n",
    "L=df_14['L']\n",
    "M=df_14['M']\n",
    "\n",
    "#Using OLS with dummy variables included production function estimates\n",
    "a=ols2.params[1]\n",
    "b=ols2.params[2]\n",
    "r=ols2.params[3]\n",
    "\n",
    "A = Y/(K**a*L**b*M**r)\n",
    "\n",
    "plt.hist(A,20) # The second argument sets the number of bins\n",
    "plt.show()\n",
    "print('\\n')\n",
    "print(A.describe(percentiles=[0.25, 0.50, 0.75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBpIkDANJEoaBJAnDQJKEYSBJAv4PhAU6E4ifzoIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1b51a630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "count    1046.000000\n",
      "mean      125.301778\n",
      "std       123.469696\n",
      "min         2.145345\n",
      "25%        71.390076\n",
      "50%       102.245647\n",
      "75%       139.900412\n",
      "max      2382.779658\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#1.5 Total Factor Productivity estimates--VA version --> The Production Estimate I Selected\n",
    "\n",
    "VA=df_1314['VA14']\n",
    "\n",
    "K=df_1314['K14']\n",
    "L=df_1314['L14']\n",
    "\n",
    "#Using Proxy Variable production function estimates\n",
    "alpha_hat = ols6.params[1]\n",
    "beta_hat = ols5.params[0]\n",
    "#Total factor productivity estimates using Proxy Variable production function estimates\n",
    "A1 = VA/(K**alpha_hat*L**beta_hat)\n",
    "\n",
    "plt.hist(A1,20) # The second argument sets the number of bins\n",
    "plt.show()\n",
    "print('\\n')\n",
    "print(A1.describe(percentiles=[0.25, 0.50, 0.75]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5Q: Interpret this ratio.\n",
    "\n",
    "A: Total factor productivity in 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>VA14</td>       <th>  R-squared:         </th> <td>   0.310</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.310</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   26.36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 09 Mar 2018</td> <th>  Prob (F-statistic):</th> <td>3.38e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:00:23</td>     <th>  Log-Likelihood:    </th> <td> -2525.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1046</td>      <th>  AIC:               </th> <td>   5053.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1045</td>      <th>  BIC:               </th> <td>   5058.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC0</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>    0.0103</td> <td>    0.002</td> <td>    5.134</td> <td> 0.000</td> <td>    0.006</td> <td>    0.014</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1328.601</td> <th>  Durbin-Watson:     </th>  <td>   1.950</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>214943.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 6.579</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>71.983</td>  <th>  Cond. No.          </th>  <td>    1.00</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   VA14   R-squared:                       0.310\n",
       "Model:                            OLS   Adj. R-squared:                  0.310\n",
       "Method:                 Least Squares   F-statistic:                     26.36\n",
       "Date:                Fri, 09 Mar 2018   Prob (F-statistic):           3.38e-07\n",
       "Time:                        16:00:23   Log-Likelihood:                -2525.4\n",
       "No. Observations:                1046   AIC:                             5053.\n",
       "Df Residuals:                    1045   BIC:                             5058.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:                  HC0                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0103      0.002      5.134      0.000       0.006       0.014\n",
       "==============================================================================\n",
       "Omnibus:                     1328.601   Durbin-Watson:                   1.950\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           214943.038\n",
       "Skew:                           6.579   Prob(JB):                         0.00\n",
       "Kurtosis:                      71.983   Cond. No.                         1.00\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
       "\"\"\""
      ]
     },
     "execution_count": 950,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.6 Productivity decomposition\n",
    "\n",
    "VA = df_1314['VA14']\n",
    "S = VA/np.mean(VA)\n",
    "\n",
    "ols7=sm.OLS(S, A1).fit(cov_type=\"HC0\")\n",
    "ols7.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6Q: Let ${S}_t = {Y}_t/E[{Y}_t]$\n",
    "Show that $E[{S}_t{A}_t] = E[{S}_t]E[{A}_t]+C({S}_t,{A}_t)$ \n",
    "                         = $E[{A}_t]$+${\\beta}$$V({A}_t)$,\n",
    "                          \n",
    "                          \n",
    "where ${\\beta}$ is the coefficient on ${A}_t$ in the best linear predictor of ${S}_t$ onto ${A}_t$. Interpret this decomposition\n",
    "\n",
    "Ans: Since $C({S}_t,{A}_t) = E[{S}_t{A}_t] - E[{S}_t]E[{A}_t]$,\n",
    "we have the first identity hold when we move the term $E[{S}_t]E[{A}_t]$ over to the left hand side and switch the sides.\n",
    "Then, since $E[{S}_t] = E[{Y}_t/E[{Y}_t]] = E[{Y}_t]/E[{Y}_t] = 1$, we have $E[{S}_t]E[{A}_t]=E[{A}_t]$\n",
    "\n",
    "Also, since ${\\beta}$ = $C({S}_t,{A}_t)$/$V({A}_t)$, we have $C({S}_t,{A}_t)$ = ${\\beta}$$V({A}_t)$\n",
    "\n",
    "Thus, $E[{S}_t]E[{A}_t]+C({S}_t,{A}_t)$ = $E[{A}_t]$+${\\beta}$$V({A}_t)$,\n",
    "and we have shown the second identity\n",
    "\n",
    "Now to interpret this decomposition, we can see $E[{S}_t{A}_t] = E[{A}_t] + C[{S}_t,{A}_t]$ first. Here, ${S}_t$ gives the relative firm size by sale, and we see how the average relative productivity of firms of different sizes is determined by adding covariance of S and ${A}_t$ to the the average productivity of all firms. For example, if the covariance is zero, then we get $E[{S}_t{A}_t]$ = $E[{A}_t]$, meaning we get the same average productivity regardless of the firm size, since firm size and productivity are not correlated. Lastly, if we believe $C[{S}_t,{A}_t]$ is positive, then the average relative productivity among larger firms will be greater than the average productivity of all firms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.30177772530027\n",
      "\n",
      "\n",
      "157.145276883\n"
     ]
    }
   ],
   "source": [
    "#1.6 Calculate estimates of its two terms for 2014\n",
    "E_A = np.mean(A1)\n",
    "beta = ols7.params[0]\n",
    "V_A = np.var(A1)\n",
    "\n",
    "print(E_A)\n",
    "print('\\n')\n",
    "print(beta*V_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7 Wrap up\n",
    "\n",
    "Q: Write 3 or 4 paragraphs summarizing analysis of this dataset.\n",
    "\n",
    "A:\n",
    "As much as firms are interested in measuring productivity, productivity measurement is a challenging task facing economists and policymakers. Since 1980's with increase in market competition, questions on productivity change have become more relevant, since they would determine the level of inequality in the market deriving from the gap of technology and skill.\n",
    "\n",
    "In this exercise we have analyzed three different ways to estimate firm's productivity. \n",
    "First, we adopted Cobb-Douglas production function form of $Y=A{K}^\\alpha{L}^\\beta{M}^\\gamma$ and took the log of both sides to run a simple OLS regression of logY on $logK$, $logL$, and $logM$. Using the data of observed input variables like labor, capital, and materials expenditures, we were then able to obtain estimates for elasticities of output with respect to capital, labor, and materials expenditures. In the second part, we added sector specific dummy variables to our previous OLS regression, thus fixing industry-specific variables. Based on the regression statistics obtained, we then performed hypothesis testing for constant returns to scale, conclduing that under 95% confidence level we reject the null hypothesis of constant returns to scale in both regressions.\n",
    "\n",
    "Then, we used \"first differenced\" approach, keeping the production function the same. For this analysis, we had to make an assumption of constant elasticity of product with respect to input variables in addition to the exogeneity assumption. The goal of this approach would be to eliminate effect of unobserved variables that remain constant over time from our analysis since that might potentially bias our estimates. In both panel data regression and panel data regression with dummy variables, we were not able to reject the null hypothesis of constant returns to scale.\n",
    "\n",
    "OLS and \"first differenced\" approaches seem to rely on stronger general econometric assumptions than the proxy variable approach. Before in OLS and panel data approaches, productivity is assumed to be uncorrelated with capital and labor, either in level terms or growth rate terms. On the other hand, the proxy variable approach adopts stronger assumptions on the economic nature of the firm's production functions. Here, productivity is given as a function of capital and investments that are slowly adjustable, while labor is rather more promptly adjusted. If these assumptions reflect the true nature of firms and market conditions, then our estimates would be closer to unbiased.\n",
    "\n",
    "Finally, we obtain total factor productivity estimates using one of the approaches mentioned above. The measure and distribution of productivity estimates vary significantly, depending on the methodology chosen. Depending on the firm size, we can decompose the average relative productivity into the average productivity and the covariance between the relative size and the productivity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
